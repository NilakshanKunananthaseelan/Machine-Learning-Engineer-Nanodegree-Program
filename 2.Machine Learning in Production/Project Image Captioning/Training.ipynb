{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from utils import train, validate, save_epoch, early_stopping\n",
    "from coco_dataloader import get_loader\n",
    "from model import ResNetEncoder, RNNDecoder\n",
    "\n",
    "batch_size=32\n",
    "vocab_threshold = 5\n",
    "load_vocab = True\n",
    "embedding_size=256\n",
    "hidden_size=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a transform to pre-process the training images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Define a transform to pre-process the validation images\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-built vocab file\n",
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 630/100000 [00:00<00:15, 6296.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "IDS 414113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7431.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader, applying the transforms\n",
    "train_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         sample_size = 100000,\n",
    "                         batch_size=batch_size,\n",
    "                         threshold=vocab_threshold,\n",
    "                         load_vocab=load_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-built vocab file\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 715/30000 [00:00<00:04, 7141.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "IDS 202654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:04<00:00, 7265.18it/s]\n"
     ]
    }
   ],
   "source": [
    "val_loader = get_loader(transform=transform_val,\n",
    "                         mode='val',\n",
    "#                          sample_size = 30000,\n",
    "                         batch_size=batch_size,\n",
    "                         threshold=vocab_threshold,\n",
    "                         load_vocab=load_vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "num_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/ec2-user/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71deebb38144f79a4c84d3cdf8beea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = ResNetEncoder(embedding_size)\n",
    "decoder = RNNDecoder(embedding_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps: 3125\n",
      "Number of validation steps: 938\n"
     ]
    }
   ],
   "source": [
    "total_train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_train_step)\n",
    "print (\"Number of validation steps:\", total_val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Step [1/3125],8.449902296066284s, Loss:8.9115\n",
      "Epoch 1,Step [2/3125],17.32950258255005s, Loss:8.8106\n",
      "Epoch 1,Step [3/3125],25.44389057159424s, Loss:8.6398\n",
      "Epoch 1,Step [4/3125],33.58589696884155s, Loss:8.3353\n"
     ]
    }
   ],
   "source": [
    "# Keep track of train and validation losses and validation Bleu-4 scores by epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_bleus = []\n",
    "# Keep track of the current best validation Bleu score\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "             break\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Step [5101/12942],5.198509693145752s, Loss:2.1459\n",
      "Epoch 1,Step [5102/12942],10.339481592178345s, Loss:2.0999\n",
      "Epoch 1,Step [5103/12942],15.504561424255371s, Loss:2.4344\n",
      "Epoch 1,Step [5104/12942],20.58127999305725s, Loss:2.0875\n",
      "Epoch 1,Step [5105/12942],25.795966148376465s, Loss:2.7829\n",
      "Epoch 1,Step [5106/12942],30.98399567604065s, Loss:2.2643\n",
      "Epoch 1,Step [5107/12942],36.169461488723755s, Loss:2.3524\n",
      "Epoch 1,Step [5108/12942],42.50917339324951s, Loss:2.6932\n",
      "Epoch 1,Step [5109/12942],51.55997371673584s, Loss:2.6480\n",
      "Epoch 1,Step [5110/12942],77.7855772972107s, Loss:2.1126\n",
      "Epoch 1,Step [5111/12942],110.40236234664917s, Loss:1.9038\n",
      "Epoch 1,Step [5112/12942],142.48601627349854s, Loss:2.2815\n",
      "Epoch 1,Step [5113/12942],174.0038137435913s, Loss:1.9853\n",
      "Epoch 1,Step [5114/12942],205.1396987438202s, Loss:2.1130\n",
      "Epoch 1,Step [5115/12942],247.2653510570526s, Loss:3.2203\n",
      "Epoch 1,Step [5116/12942],279.94119811058044s, Loss:2.7299\n",
      "Epoch 1,Step [5117/12942],296.773446559906s, Loss:2.2835\n",
      "Epoch 1,Step [5118/12942],302.0192885398865s, Loss:2.4014\n",
      "Epoch 1,Step [5119/12942],307.1938180923462s, Loss:2.1647\n",
      "Epoch 1,Step [5120/12942],312.4400067329407s, Loss:2.6983\n",
      "Epoch 1,Step [5121/12942],317.28201246261597s, Loss:2.4167\n",
      "Epoch 1,Step [5122/12942],322.5087425708771s, Loss:2.6625\n",
      "Epoch 1,Step [5123/12942],329.20601749420166s, Loss:2.2094\n",
      "Epoch 1,Step [5124/12942],354.36321687698364s, Loss:3.9540\n",
      "Epoch 1,Step [5125/12942],385.5203833580017s, Loss:2.4327\n",
      "Epoch 1,Step [5126/12942],426.27274560928345s, Loss:2.3455\n",
      "Epoch 1,Step [5127/12942],456.97548055648804s, Loss:2.6083\n",
      "Epoch 1,Step [5128/12942],488.480135679245s, Loss:2.2871\n",
      "Epoch 1,Step [5129/12942],519.3027882575989s, Loss:2.1457\n",
      "Epoch 1,Step [5130/12942],551.83469581604s, Loss:2.2866\n",
      "Epoch 1,Step [5131/12942],589.0405077934265s, Loss:2.5596\n",
      "Epoch 1,Step [5132/12942],598.807902097702s, Loss:2.1337\n",
      "Epoch 1,Step [5133/12942],604.0260741710663s, Loss:2.0572\n",
      "Epoch 1,Step [5134/12942],609.2016632556915s, Loss:2.4139\n",
      "Epoch 1,Step [5135/12942],614.462587594986s, Loss:2.9368\n",
      "Epoch 1,Step [5136/12942],619.6638643741608s, Loss:2.2656\n",
      "Epoch 1,Step [5137/12942],625.4869668483734s, Loss:2.2858\n",
      "Epoch 1,Step [5138/12942],641.7279295921326s, Loss:2.3954\n",
      "Epoch 1,Step [5139/12942],673.7346646785736s, Loss:2.1578\n",
      "Epoch 1,Step [5140/12942],706.0191252231598s, Loss:2.9011\n",
      "Epoch 1,Step [5141/12942],737.7059290409088s, Loss:2.0379\n",
      "Epoch 1,Step [5142/12942],777.8104872703552s, Loss:2.1482\n",
      "Epoch 1,Step [5143/12942],809.4710083007812s, Loss:2.4282\n",
      "Epoch 1,Step [5144/12942],840.9397566318512s, Loss:2.1676\n",
      "Epoch 1,Step [5145/12942],872.0128710269928s, Loss:2.2653\n",
      "Epoch 1,Step [5146/12942],895.3838164806366s, Loss:2.2458\n",
      "Epoch 1,Step [5147/12942],900.6121475696564s, Loss:2.1586\n",
      "Epoch 1,Step [5148/12942],905.8244976997375s, Loss:2.1402\n",
      "Epoch 1,Step [5149/12942],910.9980404376984s, Loss:1.9804\n",
      "Epoch 1,Step [5150/12942],916.2285003662109s, Loss:2.2307\n",
      "Epoch 1,Step [5151/12942],921.2608199119568s, Loss:2.2174\n",
      "Epoch 1,Step [5152/12942],927.5377571582794s, Loss:2.1563\n",
      "Epoch 1,Step [5153/12942],953.5370953083038s, Loss:2.3600\n",
      "Epoch 1,Step [5154/12942],989.1016592979431s, Loss:2.4201\n",
      "Epoch 1,Step [5155/12942],1022.0970494747162s, Loss:1.9145\n",
      "Epoch 1,Step [5156/12942],1054.6618859767914s, Loss:2.0752\n",
      "Epoch 1,Step [5157/12942],1086.6673719882965s, Loss:2.1964\n",
      "Epoch 1,Step [5158/12942],1119.5322952270508s, Loss:2.2688\n",
      "Epoch 1,Step [5159/12942],1157.574794769287s, Loss:2.1204\n",
      "Epoch 1,Step [5160/12942],1188.3118500709534s, Loss:2.5239\n",
      "Epoch 1,Step [5161/12942],1198.2503006458282s, Loss:2.1934\n",
      "Epoch 1,Step [5162/12942],1203.4771676063538s, Loss:2.2089\n",
      "Epoch 1,Step [5163/12942],1208.7387199401855s, Loss:2.1257\n",
      "Epoch 1,Step [5164/12942],1213.8940331935883s, Loss:2.0398\n",
      "Epoch 1,Step [5165/12942],1218.9043579101562s, Loss:2.3158\n",
      "Epoch 1,Step [5166/12942],1224.0390574932098s, Loss:2.2718\n",
      "Epoch 1,Step [5167/12942],1233.3721687793732s, Loss:2.3354\n",
      "Epoch 1,Step [5168/12942],1264.2391974925995s, Loss:2.5478\n",
      "Epoch 1,Step [5169/12942],1296.5707428455353s, Loss:2.2798\n",
      "Epoch 1,Step [5170/12942],1335.7111597061157s, Loss:2.1718\n",
      "Epoch 1,Step [5171/12942],1368.53063082695s, Loss:2.1719\n",
      "Epoch 1,Step [5172/12942],1402.0663847923279s, Loss:2.5184\n",
      "Epoch 1,Step [5173/12942],1432.818783044815s, Loss:2.5314\n",
      "Epoch 1,Step [5174/12942],1465.004563331604s, Loss:2.3690\n",
      "Epoch 1,Step [5175/12942],1495.9016890525818s, Loss:2.4601\n",
      "Epoch 1,Step [5176/12942],1500.7518944740295s, Loss:2.4752\n",
      "Epoch 1,Step [5177/12942],1505.8255212306976s, Loss:2.6606\n",
      "Epoch 1,Step [5178/12942],1511.028242111206s, Loss:2.0609\n",
      "Epoch 1,Step [5179/12942],1516.2387459278107s, Loss:2.4902\n",
      "Epoch 1,Step [5180/12942],1521.4186129570007s, Loss:2.2335\n",
      "Epoch 1,Step [5181/12942],1527.3465888500214s, Loss:2.1730\n",
      "Epoch 1,Step [5182/12942],1546.7667469978333s, Loss:2.1200\n",
      "Epoch 1,Step [5183/12942],1578.1028470993042s, Loss:2.3458\n",
      "Epoch 1,Step [5184/12942],1608.9179356098175s, Loss:2.4998\n",
      "Epoch 1,Step [5185/12942],1641.8344762325287s, Loss:2.1771\n",
      "Epoch 1,Step [5186/12942],1683.0692164897919s, Loss:1.9247\n",
      "Epoch 1,Step [5187/12942],1715.7056806087494s, Loss:2.4424\n",
      "Epoch 1,Step [5188/12942],1748.530900478363s, Loss:2.1065\n",
      "Epoch 1,Step [5189/12942],1780.5817441940308s, Loss:2.1513\n",
      "Epoch 1,Step [5190/12942],1796.822215795517s, Loss:2.4060\n",
      "Epoch 1,Step [5191/12942],1801.9392764568329s, Loss:2.3755\n",
      "Epoch 1,Step [5192/12942],1806.8485598564148s, Loss:2.0834\n",
      "Epoch 1,Step [5193/12942],1811.9393723011017s, Loss:2.4484\n",
      "Epoch 1,Step [5194/12942],1817.204818725586s, Loss:2.4000\n",
      "Epoch 1,Step [5195/12942],1822.3781626224518s, Loss:2.2837\n",
      "Epoch 1,Step [5196/12942],1829.365812778473s, Loss:2.0459\n",
      "Epoch 1,Step [5197/12942],1862.521476507187s, Loss:2.2991\n",
      "Epoch 1,Step [5198/12942],1892.5891799926758s, Loss:2.5859\n",
      "Epoch 1,Step [5199/12942],1926.0821793079376s, Loss:2.3426\n",
      "Epoch 1,Step [5200/12942],1957.3459854125977s, Loss:2.0522\n",
      "Epoch 1,Step [5201/12942],1987.4662573337555s, Loss:2.2625\n",
      "Epoch 1,Step [5202/12942],2022.3954055309296s, Loss:2.2205\n",
      "Epoch 1,Step [5203/12942],2059.5486578941345s, Loss:2.7492\n",
      "Epoch 1,Step [5204/12942],2092.0015575885773s, Loss:2.4164\n",
      "Epoch 1,Step [5205/12942],2098.584515810013s, Loss:2.2396\n",
      "Epoch 1,Step [5206/12942],2103.80202794075s, Loss:2.5556\n",
      "Epoch 1,Step [5207/12942],2109.1343228816986s, Loss:2.5375\n",
      "Epoch 1,Step [5208/12942],2114.384181499481s, Loss:2.7898\n",
      "Epoch 1,Step [5209/12942],2119.6297817230225s, Loss:2.2829\n",
      "Epoch 1,Step [5210/12942],2125.3052332401276s, Loss:2.2997\n",
      "Epoch 1,Step [5211/12942],2138.2018473148346s, Loss:2.4222\n",
      "Epoch 1,Step [5212/12942],2171.151750564575s, Loss:2.4089\n",
      "Epoch 1,Step [5213/12942],2208.4482662677765s, Loss:2.0473\n",
      "Epoch 1,Step [5214/12942],2245.2871611118317s, Loss:2.0824\n",
      "Epoch 1,Step [5215/12942],2276.6411657333374s, Loss:2.1685\n",
      "Epoch 1,Step [5216/12942],2310.4514133930206s, Loss:2.0667\n",
      "Epoch 1,Step [5217/12942],2342.096854686737s, Loss:2.0105\n",
      "Epoch 1,Step [5218/12942],2374.942581653595s, Loss:2.0060\n",
      "Epoch 1,Step [5219/12942],2397.5383656024933s, Loss:2.7395\n",
      "Epoch 1,Step [5220/12942],2402.7708859443665s, Loss:2.1641\n",
      "Epoch 1,Step [5221/12942],2408.120962381363s, Loss:2.3975\n",
      "Epoch 1,Step [5222/12942],2413.2320351600647s, Loss:2.1830\n",
      "Epoch 1,Step [5223/12942],2418.4297289848328s, Loss:1.7012\n",
      "Epoch 1,Step [5224/12942],2423.683316230774s, Loss:2.1942\n",
      "Epoch 1,Step [5225/12942],2433.114502429962s, Loss:2.5247\n",
      "Epoch 1,Step [5226/12942],2465.441989660263s, Loss:2.2659\n",
      "Epoch 1,Step [5227/12942],2496.360606908798s, Loss:1.9843\n",
      "Epoch 1,Step [5228/12942],2527.6583998203278s, Loss:2.0508\n",
      "Epoch 1,Step [5229/12942],2557.774189710617s, Loss:2.2479\n",
      "Epoch 1,Step [5230/12942],2597.91601896286s, Loss:2.2920\n",
      "Epoch 1,Step [5231/12942],2629.826896905899s, Loss:2.8730\n",
      "Epoch 1,Step [5232/12942],2662.5419523715973s, Loss:3.0280\n",
      "Epoch 1,Step [5233/12942],2693.890079021454s, Loss:2.2072\n",
      "Epoch 1,Step [5234/12942],2699.26860499382s, Loss:2.2156\n",
      "Epoch 1,Step [5235/12942],2704.5385417938232s, Loss:2.4379\n",
      "Epoch 1,Step [5236/12942],2709.80348610878s, Loss:1.9330\n",
      "Epoch 1,Step [5237/12942],2715.0843534469604s, Loss:2.2236\n",
      "Epoch 1,Step [5238/12942],2720.3294699192047s, Loss:2.4907\n",
      "Epoch 1,Step [5239/12942],2726.409516096115s, Loss:3.0841\n",
      "Epoch 1,Step [5240/12942],2744.3779578208923s, Loss:2.3383\n",
      "Epoch 1,Step [5241/12942],2780.5497722625732s, Loss:2.3467\n",
      "Epoch 1,Step [5242/12942],2813.938535451889s, Loss:2.5549\n",
      "Epoch 1,Step [5243/12942],2846.748416185379s, Loss:2.0004\n",
      "Epoch 1,Step [5244/12942],2879.3501307964325s, Loss:2.1335\n",
      "Epoch 1,Step [5245/12942],2910.496381044388s, Loss:2.3088\n",
      "Epoch 1,Step [5246/12942],2952.3748939037323s, Loss:2.4876\n",
      "Epoch 1,Step [5247/12942],2985.472657442093s, Loss:2.1701\n",
      "Epoch 1,Step [5248/12942],2997.7691197395325s, Loss:2.1169\n",
      "Epoch 1,Step [5249/12942],3003.076775789261s, Loss:2.4102\n",
      "Epoch 1,Step [5250/12942],3008.3046486377716s, Loss:2.2019\n",
      "Epoch 1,Step [5251/12942],3013.5654542446136s, Loss:2.0188\n",
      "Epoch 1,Step [5252/12942],3018.781461238861s, Loss:2.2335\n",
      "Epoch 1,Step [5253/12942],3024.164256811142s, Loss:2.1985\n",
      "Epoch 1,Step [5254/12942],3033.7618980407715s, Loss:2.1707\n",
      "Epoch 1,Step [5255/12942],3066.6390149593353s, Loss:2.1485\n",
      "Epoch 1,Step [5256/12942],3099.529489994049s, Loss:2.4492\n",
      "Epoch 1,Step [5257/12942],3139.047020673752s, Loss:2.1122\n",
      "Epoch 1,Step [5258/12942],3170.004665851593s, Loss:2.1786\n",
      "Epoch 1,Step [5259/12942],3203.4979140758514s, Loss:2.7305\n",
      "Epoch 1,Step [5260/12942],3237.333984851837s, Loss:2.1556\n",
      "Epoch 1,Step [5261/12942],3270.058407306671s, Loss:2.4504\n",
      "Epoch 1,Step [5262/12942],3296.7242267131805s, Loss:2.2300\n",
      "Epoch 1,Step [5263/12942],3301.9562001228333s, Loss:1.9437\n",
      "Epoch 1,Step [5264/12942],3307.22088599205s, Loss:2.3207\n",
      "Epoch 1,Step [5265/12942],3312.366326570511s, Loss:2.1797\n",
      "Epoch 1,Step [5266/12942],3317.667642354965s, Loss:2.2685\n",
      "Epoch 1,Step [5267/12942],3322.9039738178253s, Loss:2.2773\n",
      "Epoch 1,Step [5268/12942],3330.8153097629547s, Loss:2.2840\n",
      "Epoch 1,Step [5269/12942],3357.0931141376495s, Loss:2.1981\n",
      "Epoch 1,Step [5270/12942],3387.7326555252075s, Loss:2.0850\n",
      "Epoch 1,Step [5271/12942],3419.8075387477875s, Loss:2.1153\n",
      "Epoch 1,Step [5272/12942],3450.187012910843s, Loss:2.4897\n",
      "Epoch 1,Step [5273/12942],3489.6271421909332s, Loss:2.2288\n",
      "Epoch 1,Step [5274/12942],3520.836259365082s, Loss:2.1717\n",
      "Epoch 1,Step [5275/12942],3552.065500974655s, Loss:2.1784\n",
      "Epoch 1,Step [5276/12942],3583.4016647338867s, Loss:2.4432\n",
      "Epoch 1,Step [5277/12942],3597.031980752945s, Loss:2.2287\n",
      "Epoch 1,Step [5278/12942],3602.0904743671417s, Loss:1.9903\n",
      "Epoch 1,Step [5279/12942],3607.3251032829285s, Loss:2.0966\n",
      "Epoch 1,Step [5280/12942],3612.6011171340942s, Loss:2.2614\n",
      "Epoch 1,Step [5281/12942],3617.817722558975s, Loss:2.1642\n",
      "Epoch 1,Step [5282/12942],3623.1277408599854s, Loss:2.7221\n",
      "Epoch 1,Step [5283/12942],3631.283267021179s, Loss:2.2439\n",
      "Epoch 1,Step [5284/12942],3667.533692598343s, Loss:2.1745\n",
      "Epoch 1,Step [5285/12942],3700.531263113022s, Loss:2.4057\n",
      "Epoch 1,Step [5286/12942],3731.5292196273804s, Loss:2.3290\n",
      "Epoch 1,Step [5287/12942],3764.049298763275s, Loss:2.3116\n",
      "Epoch 1,Step [5288/12942],3796.8880121707916s, Loss:1.9198\n",
      "Epoch 1,Step [5289/12942],3835.592445373535s, Loss:2.1185\n",
      "Epoch 1,Step [5290/12942],3870.024279356003s, Loss:2.2607\n",
      "Epoch 1,Step [5291/12942],3894.8486585617065s, Loss:2.4343\n",
      "Epoch 1,Step [5292/12942],3900.0725359916687s, Loss:2.0816\n",
      "Epoch 1,Step [5293/12942],3905.003932237625s, Loss:2.1517\n",
      "Epoch 1,Step [5294/12942],3910.222160100937s, Loss:2.5651\n",
      "Epoch 1,Step [5295/12942],3915.4351904392242s, Loss:2.3917\n",
      "Epoch 1,Step [5296/12942],3920.374473810196s, Loss:2.3777\n",
      "Epoch 1,Step [5297/12942],3926.1233298778534s, Loss:2.1886\n",
      "Epoch 1,Step [5298/12942],3940.226986885071s, Loss:2.1327\n",
      "Epoch 1,Step [5299/12942],3970.524137020111s, Loss:2.5792\n",
      "Epoch 1,Step [5300/12942],4005.2856459617615s, Loss:2.3743\n",
      "Epoch 1,Step [5301/12942],4045.099937915802s, Loss:2.7296\n",
      "Epoch 1,Step [5302/12942],4076.752133369446s, Loss:2.2130\n",
      "Epoch 1,Step [5303/12942],4109.056809663773s, Loss:2.1321\n",
      "Epoch 1,Step [5304/12942],4142.107480287552s, Loss:2.0491\n",
      "Epoch 1,Step [5305/12942],4174.355540513992s, Loss:2.0786\n",
      "Epoch 1,Step [5306/12942],4197.551359653473s, Loss:2.1583\n",
      "Epoch 1,Step [5307/12942],4202.5625693798065s, Loss:2.3369\n",
      "Epoch 1,Step [5308/12942],4207.399203777313s, Loss:2.2589\n",
      "Epoch 1,Step [5309/12942],4212.530948877335s, Loss:2.3222\n",
      "Epoch 1,Step [5310/12942],4217.7721474170685s, Loss:2.3843\n",
      "Epoch 1,Step [5311/12942],4222.9946048259735s, Loss:2.0840\n",
      "Epoch 1,Step [5312/12942],4231.370544433594s, Loss:2.2728\n",
      "Epoch 1,Step [5313/12942],4259.499829530716s, Loss:2.2984\n",
      "Epoch 1,Step [5314/12942],4291.702291965485s, Loss:2.7076\n",
      "Epoch 1,Step [5315/12942],4323.481044054031s, Loss:2.0590\n",
      "Epoch 1,Step [5316/12942],4355.63272356987s, Loss:2.5750\n",
      "Epoch 1,Step [5317/12942],4395.292120218277s, Loss:1.9415\n",
      "Epoch 1,Step [5318/12942],4426.261053562164s, Loss:2.4453\n",
      "Epoch 1,Step [5319/12942],4459.052543878555s, Loss:2.1102\n",
      "Epoch 1,Step [5320/12942],4490.983963727951s, Loss:2.5398\n",
      "Epoch 1,Step [5321/12942],4498.571316719055s, Loss:2.2763\n",
      "Epoch 1,Step [5322/12942],4503.926795959473s, Loss:2.6603\n",
      "Epoch 1,Step [5323/12942],4509.210534334183s, Loss:2.0311\n",
      "Epoch 1,Step [5324/12942],4514.26109957695s, Loss:2.3305\n",
      "Epoch 1,Step [5325/12942],4519.502001047134s, Loss:2.0136\n",
      "Epoch 1,Step [5326/12942],4525.088866472244s, Loss:1.9589\n",
      "Epoch 1,Step [5327/12942],4536.0560393333435s, Loss:1.9495\n",
      "Epoch 1,Step [5328/12942],4576.018517494202s, Loss:2.6016\n",
      "Epoch 1,Step [5329/12942],4608.339008808136s, Loss:2.5284\n",
      "Epoch 1,Step [5330/12942],4640.797745704651s, Loss:2.2058\n",
      "Epoch 1,Step [5331/12942],4673.82418012619s, Loss:2.5136\n",
      "Epoch 1,Step [5332/12942],4706.672780275345s, Loss:2.3697\n",
      "Epoch 1,Step [5333/12942],4747.663692712784s, Loss:2.3051\n",
      "Epoch 1,Step [5334/12942],4781.216813802719s, Loss:1.8541\n",
      "Epoch 1,Step [5335/12942],4796.985361099243s, Loss:2.3209\n",
      "Epoch 1,Step [5336/12942],4802.215467214584s, Loss:2.3234\n",
      "Epoch 1,Step [5337/12942],4807.398708343506s, Loss:2.6520\n",
      "Epoch 1,Step [5338/12942],4812.627446174622s, Loss:2.6995\n",
      "Epoch 1,Step [5339/12942],4817.893337249756s, Loss:2.5483\n",
      "Epoch 1,Step [5340/12942],4823.050504922867s, Loss:2.1378\n",
      "Epoch 1,Step [5341/12942],4830.966108083725s, Loss:2.3487\n",
      "Epoch 1,Step [5342/12942],4858.830769538879s, Loss:1.9815\n",
      "Epoch 1,Step [5343/12942],4891.900564193726s, Loss:2.4473\n",
      "Epoch 1,Step [5344/12942],4932.906621932983s, Loss:2.4810\n",
      "Epoch 1,Step [5345/12942],4964.487725019455s, Loss:2.4518\n",
      "Epoch 1,Step [5346/12942],4995.768898010254s, Loss:2.8112\n",
      "Epoch 1,Step [5347/12942],5026.553619146347s, Loss:2.2322\n",
      "Epoch 1,Step [5348/12942],5057.326532363892s, Loss:2.2216\n",
      "Epoch 1,Step [5349/12942],5094.317424297333s, Loss:2.7634\n",
      "Epoch 1,Step [5350/12942],5099.75799870491s, Loss:2.3313\n",
      "Epoch 1,Step [5351/12942],5105.057765483856s, Loss:2.5228\n",
      "Epoch 1,Step [5352/12942],5109.91441655159s, Loss:2.1480\n",
      "Epoch 1,Step [5353/12942],5114.996882677078s, Loss:2.4405\n",
      "Epoch 1,Step [5354/12942],5120.211594104767s, Loss:2.3022\n",
      "Epoch 1,Step [5355/12942],5125.84118938446s, Loss:2.3411\n",
      "Epoch 1,Step [5356/12942],5139.41446852684s, Loss:2.4756\n",
      "Epoch 1,Step [5357/12942],5170.70919251442s, Loss:3.2644\n",
      "Epoch 1,Step [5358/12942],5201.233501672745s, Loss:2.2218\n",
      "Epoch 1,Step [5359/12942],5235.7092888355255s, Loss:2.7906\n",
      "Epoch 1,Step [5360/12942],5273.931172370911s, Loss:2.1735\n",
      "Epoch 1,Step [5361/12942],5306.341552495956s, Loss:2.3701\n",
      "Epoch 1,Step [5362/12942],5339.2197597026825s, Loss:2.1730\n",
      "Epoch 1,Step [5363/12942],5371.662881612778s, Loss:2.1783\n",
      "Epoch 1,Step [5364/12942],5395.330838680267s, Loss:2.2536\n",
      "Epoch 1,Step [5365/12942],5400.547035217285s, Loss:2.1100\n",
      "Epoch 1,Step [5366/12942],5405.563623428345s, Loss:2.0304\n",
      "Epoch 1,Step [5367/12942],5410.809334278107s, Loss:2.2129\n",
      "Epoch 1,Step [5368/12942],5416.071928501129s, Loss:2.2984\n",
      "Epoch 1,Step [5369/12942],5420.922231674194s, Loss:2.0237\n",
      "Epoch 1,Step [5370/12942],5426.9597260952s, Loss:2.6289\n",
      "Epoch 1,Step [5371/12942],5448.736418247223s, Loss:2.4216\n",
      "Epoch 1,Step [5372/12942],5482.092218399048s, Loss:2.3503\n",
      "Epoch 1,Step [5373/12942],5513.941455364227s, Loss:2.2523\n",
      "Epoch 1,Step [5374/12942],5544.813627243042s, Loss:2.2185\n",
      "Epoch 1,Step [5375/12942],5577.786429166794s, Loss:2.3650\n",
      "Epoch 1,Step [5376/12942],5608.751286506653s, Loss:2.0676\n",
      "Epoch 1,Step [5377/12942],5649.757817745209s, Loss:2.1989\n",
      "Epoch 1,Step [5378/12942],5681.912159442902s, Loss:2.4517\n",
      "Epoch 1,Step [5379/12942],5696.958944797516s, Loss:2.2557\n",
      "Epoch 1,Step [5380/12942],5702.173891544342s, Loss:2.2692\n",
      "Epoch 1,Step [5381/12942],5707.400719642639s, Loss:2.0803\n",
      "Epoch 1,Step [5382/12942],5712.5925533771515s, Loss:2.0111\n",
      "Epoch 1,Step [5383/12942],5717.357959508896s, Loss:2.0448\n",
      "Epoch 1,Step [5384/12942],5722.517142295837s, Loss:2.3071\n",
      "Epoch 1,Step [5385/12942],5729.629855632782s, Loss:2.4403\n",
      "Epoch 1,Step [5386/12942],5751.183125257492s, Loss:2.0497\n",
      "Epoch 1,Step [5387/12942],5782.542696714401s, Loss:2.3065\n",
      "Epoch 1,Step [5388/12942],5822.54186964035s, Loss:2.4134\n",
      "Epoch 1,Step [5389/12942],5855.542741060257s, Loss:1.9974\n",
      "Epoch 1,Step [5390/12942],5886.719362020493s, Loss:2.4874\n",
      "Epoch 1,Step [5391/12942],5919.084054470062s, Loss:2.4953\n",
      "Epoch 1,Step [5392/12942],5951.302171707153s, Loss:2.5200\n",
      "Epoch 1,Step [5393/12942],5983.821430206299s, Loss:2.0848\n",
      "Epoch 1,Step [5394/12942],5998.60733127594s, Loss:2.5122\n",
      "Epoch 1,Step [5395/12942],6003.776707410812s, Loss:2.1273\n",
      "Epoch 1,Step [5396/12942],6009.027186632156s, Loss:2.0056\n",
      "Epoch 1,Step [5397/12942],6014.099924564362s, Loss:2.2464\n",
      "Epoch 1,Step [5398/12942],6019.312961578369s, Loss:2.4981\n",
      "Epoch 1,Step [5399/12942],6024.324687004089s, Loss:3.1351\n",
      "Epoch 1,Step [5400/12942],6033.253291606903s, Loss:2.1300\n",
      "Epoch 1,Step [5400/12942],6033.26398229599s, Loss:2.1300\n",
      "\n",
      "Epoch 1,Step [5401/12942],31.118667125701904s, Loss:2.4309\n",
      "Epoch 1,Step [5402/12942],63.28179597854614s, Loss:2.5140\n",
      "Epoch 1,Step [5403/12942],95.98689579963684s, Loss:2.4188\n",
      "Epoch 1,Step [5404/12942],129.89041352272034s, Loss:2.3383\n",
      "Epoch 1,Step [5405/12942],166.22130298614502s, Loss:4.3070\n",
      "Epoch 1,Step [5406/12942],195.85219550132751s, Loss:2.4130\n",
      "Epoch 1,Step [5407/12942],227.59400010108948s, Loss:2.3382\n",
      "Epoch 1,Step [5408/12942],259.9294228553772s, Loss:2.0295\n",
      "Epoch 1,Step [5409/12942],265.39137029647827s, Loss:2.6338\n",
      "Epoch 1,Step [5410/12942],270.6451323032379s, Loss:2.3367\n",
      "Epoch 1,Step [5411/12942],275.8991119861603s, Loss:2.1171\n",
      "Epoch 1,Step [5412/12942],280.7723054885864s, Loss:2.1011\n",
      "Epoch 1,Step [5413/12942],285.8751986026764s, Loss:1.9383\n",
      "Epoch 1,Step [5414/12942],291.5473053455353s, Loss:2.4439\n",
      "Epoch 1,Step [5415/12942],300.9249413013458s, Loss:2.0526\n",
      "Epoch 1,Step [5416/12942],339.3975079059601s, Loss:2.2408\n",
      "Epoch 1,Step [5417/12942],371.8203001022339s, Loss:2.1195\n",
      "Epoch 1,Step [5418/12942],403.29421210289s, Loss:2.2888\n",
      "Epoch 1,Step [5419/12942],434.6315941810608s, Loss:2.3023\n",
      "Epoch 1,Step [5420/12942],465.6427872180939s, Loss:2.1446\n",
      "Epoch 1,Step [5421/12942],506.8479447364807s, Loss:2.1691\n",
      "Epoch 1,Step [5422/12942],539.7138211727142s, Loss:2.4837\n",
      "Epoch 1,Step [5423/12942],557.6760923862457s, Loss:2.3760\n",
      "Epoch 1,Step [5424/12942],562.7719488143921s, Loss:2.2272\n",
      "Epoch 1,Step [5425/12942],568.1000428199768s, Loss:2.5125\n",
      "Epoch 1,Step [5426/12942],573.3029732704163s, Loss:2.2252\n",
      "Epoch 1,Step [5427/12942],578.5464692115784s, Loss:2.3450\n",
      "Epoch 1,Step [5428/12942],583.7200446128845s, Loss:2.1890\n",
      "Epoch 1,Step [5429/12942],590.4232943058014s, Loss:2.0921\n",
      "Epoch 1,Step [5430/12942],613.5371956825256s, Loss:2.0704\n",
      "Epoch 1,Step [5431/12942],646.9715645313263s, Loss:2.0396\n",
      "Epoch 1,Step [5432/12942],687.6456689834595s, Loss:2.3432\n",
      "Epoch 1,Step [5433/12942],719.8066413402557s, Loss:2.1454\n",
      "Epoch 1,Step [5434/12942],753.5031278133392s, Loss:2.4833\n",
      "Epoch 1,Step [5435/12942],785.4801115989685s, Loss:1.9647\n",
      "Epoch 1,Step [5436/12942],814.1909756660461s, Loss:2.0033\n",
      "Epoch 1,Step [5437/12942],847.4301240444183s, Loss:1.9627\n",
      "Epoch 1,Step [5438/12942],860.682160615921s, Loss:2.3595\n",
      "Epoch 1,Step [5439/12942],865.8497776985168s, Loss:2.0794\n",
      "Epoch 1,Step [5440/12942],871.1303744316101s, Loss:2.1850\n",
      "Epoch 1,Step [5441/12942],876.1217975616455s, Loss:2.1715\n",
      "Epoch 1,Step [5442/12942],881.2963001728058s, Loss:2.1433\n",
      "Epoch 1,Step [5443/12942],886.9124205112457s, Loss:2.3561\n",
      "Epoch 1,Step [5444/12942],899.9874761104584s, Loss:2.8047\n",
      "Epoch 1,Step [5445/12942],931.6250987052917s, Loss:2.3511\n",
      "Epoch 1,Step [5446/12942],963.5029332637787s, Loss:2.2148\n",
      "Epoch 1,Step [5447/12942],994.128509759903s, Loss:2.4835\n",
      "Epoch 1,Step [5448/12942],1027.1924040317535s, Loss:2.9796\n",
      "Epoch 1,Step [5449/12942],1064.453033208847s, Loss:2.2643\n",
      "Epoch 1,Step [5450/12942],1096.1805362701416s, Loss:2.4106\n",
      "Epoch 1,Step [5451/12942],1129.2066071033478s, Loss:2.2576\n",
      "Epoch 1,Step [5452/12942],1156.2112529277802s, Loss:2.2796\n",
      "Epoch 1,Step [5453/12942],1161.0913219451904s, Loss:2.9107\n",
      "Epoch 1,Step [5454/12942],1166.2902834415436s, Loss:2.3224\n",
      "Epoch 1,Step [5455/12942],1171.1303203105927s, Loss:2.1820\n",
      "Epoch 1,Step [5456/12942],1176.3146426677704s, Loss:2.4872\n",
      "Epoch 1,Step [5457/12942],1181.5267465114594s, Loss:2.3188\n",
      "Epoch 1,Step [5458/12942],1187.3941221237183s, Loss:2.0628\n",
      "Epoch 1,Step [5459/12942],1201.1279876232147s, Loss:2.1674\n",
      "Epoch 1,Step [5460/12942],1241.9371271133423s, Loss:2.4980\n",
      "Epoch 1,Step [5461/12942],1273.9520056247711s, Loss:2.1568\n",
      "Epoch 1,Step [5462/12942],1305.383442401886s, Loss:2.4401\n",
      "Epoch 1,Step [5463/12942],1337.0101675987244s, Loss:2.1396\n",
      "Epoch 1,Step [5464/12942],1369.2460646629333s, Loss:2.5438\n",
      "Epoch 1,Step [5465/12942],1407.8508462905884s, Loss:2.0961\n",
      "Epoch 1,Step [5466/12942],1440.076568365097s, Loss:2.3055\n",
      "Epoch 1,Step [5467/12942],1458.0638506412506s, Loss:2.3360\n",
      "Epoch 1,Step [5468/12942],1463.2285451889038s, Loss:2.4422\n",
      "Epoch 1,Step [5469/12942],1468.4530355930328s, Loss:1.9389\n",
      "Epoch 1,Step [5470/12942],1473.454627752304s, Loss:2.1833\n",
      "Epoch 1,Step [5471/12942],1478.6600036621094s, Loss:2.2201\n",
      "Epoch 1,Step [5472/12942],1483.8757998943329s, Loss:2.0857\n",
      "Epoch 1,Step [5473/12942],1491.1157486438751s, Loss:2.2536\n",
      "Epoch 1,Step [5474/12942],1515.646050453186s, Loss:2.3437\n",
      "Epoch 1,Step [5475/12942],1545.1803135871887s, Loss:2.1358\n",
      "Epoch 1,Step [5476/12942],1584.2576260566711s, Loss:2.5025\n",
      "Epoch 1,Step [5477/12942],1616.825581073761s, Loss:2.0809\n",
      "Epoch 1,Step [5478/12942],1649.0908825397491s, Loss:2.4968\n",
      "Epoch 1,Step [5479/12942],1681.5821361541748s, Loss:2.2556\n",
      "Epoch 1,Step [5480/12942],1713.5263826847076s, Loss:2.2847\n",
      "Epoch 1,Step [5481/12942],1748.436759710312s, Loss:2.1992\n",
      "Epoch 1,Step [5482/12942],1760.961008310318s, Loss:2.2516\n",
      "Epoch 1,Step [5483/12942],1766.1835691928864s, Loss:2.5554\n",
      "Epoch 1,Step [5484/12942],1771.055288553238s, Loss:2.1472\n",
      "Epoch 1,Step [5485/12942],1776.2170722484589s, Loss:2.3516\n",
      "Epoch 1,Step [5486/12942],1781.4165303707123s, Loss:2.1358\n",
      "Epoch 1,Step [5487/12942],1787.099562883377s, Loss:2.2445\n",
      "Epoch 1,Step [5488/12942],1800.7249870300293s, Loss:1.9604\n",
      "Epoch 1,Step [5489/12942],1833.0586977005005s, Loss:2.3019\n",
      "Epoch 1,Step [5490/12942],1866.61266040802s, Loss:2.1717\n",
      "Epoch 1,Step [5491/12942],1900.5967466831207s, Loss:2.9042\n",
      "Epoch 1,Step [5492/12942],1937.426067829132s, Loss:2.0395\n",
      "Epoch 1,Step [5493/12942],1973.4027512073517s, Loss:2.8125\n",
      "Epoch 1,Step [5494/12942],2005.4288754463196s, Loss:2.4638\n",
      "Epoch 1,Step [5495/12942],2037.5523481369019s, Loss:2.1484\n",
      "Epoch 1,Step [5496/12942],2057.8169360160828s, Loss:2.1961\n",
      "Epoch 1,Step [5497/12942],2062.9745557308197s, Loss:2.2933\n",
      "Epoch 1,Step [5498/12942],2068.231125831604s, Loss:2.2818\n",
      "Epoch 1,Step [5499/12942],2073.037261724472s, Loss:2.0224\n",
      "Epoch 1,Step [5500/12942],2078.1986248493195s, Loss:2.2508\n",
      "Epoch 1,Step [5501/12942],2083.2773728370667s, Loss:2.0619\n",
      "Epoch 1,Step [5502/12942],2089.747896671295s, Loss:2.6138\n",
      "Epoch 1,Step [5503/12942],2116.764288187027s, Loss:2.2941\n",
      "Epoch 1,Step [5504/12942],2151.3250062465668s, Loss:2.5156\n",
      "Epoch 1,Step [5505/12942],2183.5461950302124s, Loss:2.0536\n",
      "Epoch 1,Step [5506/12942],2216.702350616455s, Loss:2.4505\n",
      "Epoch 1,Step [5507/12942],2250.2989060878754s, Loss:3.6790\n",
      "Epoch 1,Step [5508/12942],2282.0861434936523s, Loss:2.2029\n",
      "Epoch 1,Step [5509/12942],2320.537811756134s, Loss:2.0952\n",
      "Epoch 1,Step [5510/12942],2351.97096824646s, Loss:2.3052\n",
      "Epoch 1,Step [5511/12942],2359.9741945266724s, Loss:2.1668\n",
      "Epoch 1,Step [5512/12942],2365.206247329712s, Loss:2.5017\n",
      "Epoch 1,Step [5513/12942],2370.3651206493378s, Loss:2.3763\n",
      "Epoch 1,Step [5514/12942],2375.1911540031433s, Loss:2.0530\n",
      "Epoch 1,Step [5515/12942],2380.425219774246s, Loss:2.4667\n",
      "Epoch 1,Step [5516/12942],2385.349338054657s, Loss:2.2671\n",
      "Epoch 1,Step [5517/12942],2393.946010351181s, Loss:2.1802\n",
      "Epoch 1,Step [5518/12942],2421.758575439453s, Loss:2.1196\n",
      "Epoch 1,Step [5519/12942],2453.9048075675964s, Loss:2.2103\n",
      "Epoch 1,Step [5520/12942],2494.5110161304474s, Loss:2.2043\n",
      "Epoch 1,Step [5521/12942],2526.7750294208527s, Loss:2.4138\n",
      "Epoch 1,Step [5522/12942],2559.83447933197s, Loss:1.9993\n",
      "Epoch 1,Step [5523/12942],2590.1102538108826s, Loss:1.9720\n",
      "Epoch 1,Step [5524/12942],2621.733255624771s, Loss:2.1386\n",
      "Epoch 1,Step [5525/12942],2655.848398923874s, Loss:2.7581\n",
      "Epoch 1,Step [5526/12942],2662.006192445755s, Loss:2.5387\n",
      "Epoch 1,Step [5527/12942],2667.210787296295s, Loss:2.5031\n",
      "Epoch 1,Step [5528/12942],2672.339022874832s, Loss:3.1106\n",
      "Epoch 1,Step [5529/12942],2677.4484000205994s, Loss:1.9812\n",
      "Epoch 1,Step [5530/12942],2682.496167898178s, Loss:2.3562\n",
      "Epoch 1,Step [5531/12942],2688.4813256263733s, Loss:2.0148\n",
      "Epoch 1,Step [5532/12942],2705.2991259098053s, Loss:2.2427\n",
      "Epoch 1,Step [5533/12942],2737.3221447467804s, Loss:2.3065\n",
      "Epoch 1,Step [5534/12942],2768.8975746631622s, Loss:2.2956\n",
      "Epoch 1,Step [5535/12942],2801.4552929401398s, Loss:2.2734\n",
      "Epoch 1,Step [5536/12942],2840.0720789432526s, Loss:2.5510\n",
      "Epoch 1,Step [5537/12942],2873.4385993480682s, Loss:2.4143\n",
      "Epoch 1,Step [5538/12942],2902.6883709430695s, Loss:2.0370\n",
      "Epoch 1,Step [5539/12942],2934.528063774109s, Loss:2.1781\n",
      "Epoch 1,Step [5540/12942],2957.1805033683777s, Loss:1.9374\n",
      "Epoch 1,Step [5541/12942],2962.3866567611694s, Loss:2.3006\n",
      "Epoch 1,Step [5542/12942],2967.597862958908s, Loss:2.1521\n",
      "Epoch 1,Step [5543/12942],2972.8032145500183s, Loss:2.4652\n",
      "Epoch 1,Step [5544/12942],2977.978274345398s, Loss:2.1282\n",
      "Epoch 1,Step [5545/12942],2983.265341758728s, Loss:2.9501\n",
      "Epoch 1,Step [5546/12942],2989.7782423496246s, Loss:2.3346\n",
      "Epoch 1,Step [5547/12942],3015.95121049881s, Loss:2.4987\n",
      "Epoch 1,Step [5548/12942],3052.722461462021s, Loss:2.3569\n",
      "Epoch 1,Step [5549/12942],3084.02286028862s, Loss:2.1566\n",
      "Epoch 1,Step [5550/12942],3115.868971824646s, Loss:2.5253\n",
      "Epoch 1,Step [5551/12942],3149.097097158432s, Loss:2.7984\n",
      "Epoch 1,Step [5552/12942],3181.812844276428s, Loss:1.9435\n",
      "Epoch 1,Step [5553/12942],3222.391041278839s, Loss:2.3535\n",
      "Epoch 1,Step [5554/12942],3254.83784532547s, Loss:2.1253\n",
      "Epoch 1,Step [5555/12942],3260.0942192077637s, Loss:2.3591\n",
      "Epoch 1,Step [5556/12942],3265.2952830791473s, Loss:2.0806\n",
      "Epoch 1,Step [5557/12942],3270.462867975235s, Loss:2.2162\n",
      "Epoch 1,Step [5558/12942],3275.7039833068848s, Loss:2.3666\n",
      "Epoch 1,Step [5559/12942],3280.8268768787384s, Loss:2.4367\n",
      "Epoch 1,Step [5560/12942],3286.25021648407s, Loss:2.0915\n",
      "Epoch 1,Step [5561/12942],3297.294219970703s, Loss:2.0294\n",
      "Epoch 1,Step [5562/12942],3328.079024076462s, Loss:2.0060\n",
      "Epoch 1,Step [5563/12942],3361.0091993808746s, Loss:2.1964\n",
      "Epoch 1,Step [5564/12942],3399.5952246189117s, Loss:2.3911\n",
      "Epoch 1,Step [5565/12942],3430.7525413036346s, Loss:2.3191\n",
      "Epoch 1,Step [5566/12942],3461.053874015808s, Loss:2.5045\n",
      "Epoch 1,Step [5567/12942],3492.811128616333s, Loss:2.2535\n",
      "Epoch 1,Step [5568/12942],3524.5584466457367s, Loss:2.2226\n",
      "Epoch 1,Step [5569/12942],3556.333242416382s, Loss:2.1160\n",
      "Epoch 1,Step [5570/12942],3562.2608563899994s, Loss:2.3464\n",
      "Epoch 1,Step [5571/12942],3567.5131771564484s, Loss:2.3350\n",
      "Epoch 1,Step [5572/12942],3572.6884846687317s, Loss:2.2176\n",
      "Epoch 1,Step [5573/12942],3577.8593254089355s, Loss:2.2880\n",
      "Epoch 1,Step [5574/12942],3583.1339337825775s, Loss:2.4523\n",
      "Epoch 1,Step [5575/12942],3589.6135680675507s, Loss:2.3017\n",
      "Epoch 1,Step [5576/12942],3611.1313378810883s, Loss:2.1662\n",
      "Epoch 1,Step [5577/12942],3643.2602877616882s, Loss:2.1991\n",
      "Epoch 1,Step [5578/12942],3676.1130878925323s, Loss:2.2224\n",
      "Epoch 1,Step [5579/12942],3706.913532972336s, Loss:2.1401\n",
      "Epoch 1,Step [5580/12942],3746.2057106494904s, Loss:2.6253\n",
      "Epoch 1,Step [5581/12942],3779.0138885974884s, Loss:2.3683\n",
      "Epoch 1,Step [5582/12942],3811.3833134174347s, Loss:2.7831\n",
      "Epoch 1,Step [5583/12942],3843.255490541458s, Loss:2.1479\n",
      "Epoch 1,Step [5584/12942],3858.5690772533417s, Loss:2.1568\n",
      "Epoch 1,Step [5585/12942],3863.4423038959503s, Loss:2.3584\n",
      "Epoch 1,Step [5586/12942],3868.599867582321s, Loss:2.1636\n",
      "Epoch 1,Step [5587/12942],3873.884035587311s, Loss:2.5507\n",
      "Epoch 1,Step [5588/12942],3878.8203806877136s, Loss:2.2391\n",
      "Epoch 1,Step [5589/12942],3883.672055721283s, Loss:2.2542\n",
      "Epoch 1,Step [5590/12942],3889.989750146866s, Loss:2.0808\n",
      "Epoch 1,Step [5591/12942],3916.955357313156s, Loss:2.1492\n",
      "Epoch 1,Step [5592/12942],3953.758090734482s, Loss:2.1517\n",
      "Epoch 1,Step [5593/12942],3984.5332260131836s, Loss:2.3709\n",
      "Epoch 1,Step [5594/12942],4017.3201541900635s, Loss:2.0493\n",
      "Epoch 1,Step [5595/12942],4048.97061419487s, Loss:2.3127\n",
      "Epoch 1,Step [5596/12942],4080.76758480072s, Loss:2.1254\n",
      "Epoch 1,Step [5597/12942],4121.662470340729s, Loss:1.9778\n",
      "Epoch 1,Step [5598/12942],4154.3011338710785s, Loss:2.5450\n",
      "Epoch 1,Step [5599/12942],4160.319764614105s, Loss:2.5101\n",
      "Epoch 1,Step [5600/12942],4165.4016370773315s, Loss:2.0975\n",
      "Epoch 1,Step [5601/12942],4170.629775524139s, Loss:1.9814\n",
      "Epoch 1,Step [5602/12942],4175.870050907135s, Loss:2.2172\n",
      "Epoch 1,Step [5603/12942],4181.100696563721s, Loss:2.1246\n",
      "Epoch 1,Step [5604/12942],4186.699546813965s, Loss:2.0351\n",
      "Epoch 1,Step [5605/12942],4199.066466093063s, Loss:2.4081\n",
      "Epoch 1,Step [5606/12942],4232.457008838654s, Loss:2.1523\n",
      "Epoch 1,Step [5607/12942],4264.854634046555s, Loss:2.6068\n",
      "Epoch 1,Step [5608/12942],4304.492884397507s, Loss:2.7096\n",
      "Epoch 1,Step [5609/12942],4335.781603813171s, Loss:2.1101\n",
      "Epoch 1,Step [5610/12942],4368.417038202286s, Loss:1.9067\n",
      "Epoch 1,Step [5611/12942],4399.576935052872s, Loss:2.0152\n",
      "Epoch 1,Step [5612/12942],4430.603776216507s, Loss:2.2819\n",
      "Epoch 1,Step [5613/12942],4457.568542003632s, Loss:2.3620\n",
      "Epoch 1,Step [5614/12942],4463.163584470749s, Loss:2.5670\n",
      "Epoch 1,Step [5615/12942],4468.208714008331s, Loss:2.5102\n",
      "Epoch 1,Step [5616/12942],4473.215359687805s, Loss:2.2605\n",
      "Epoch 1,Step [5617/12942],4478.443390607834s, Loss:2.2380\n",
      "Epoch 1,Step [5618/12942],4483.6603627204895s, Loss:2.2907\n",
      "Epoch 1,Step [5619/12942],4490.3425006866455s, Loss:2.0136\n",
      "Epoch 1,Step [5620/12942],4513.442613124847s, Loss:2.1644\n",
      "Epoch 1,Step [5621/12942],4545.784695625305s, Loss:2.3203\n",
      "Epoch 1,Step [5622/12942],4575.417739152908s, Loss:2.2390\n",
      "Epoch 1,Step [5623/12942],4607.792064189911s, Loss:1.9455\n",
      "Epoch 1,Step [5624/12942],4648.626232624054s, Loss:2.1607\n",
      "Epoch 1,Step [5625/12942],4680.609960079193s, Loss:1.8076\n",
      "Epoch 1,Step [5626/12942],4713.615327835083s, Loss:2.2472\n",
      "Epoch 1,Step [5627/12942],4745.731472730637s, Loss:2.4300\n",
      "Epoch 1,Step [5628/12942],4759.274883508682s, Loss:2.6376\n",
      "Epoch 1,Step [5629/12942],4764.215217590332s, Loss:2.2388\n",
      "Epoch 1,Step [5630/12942],4769.16388630867s, Loss:2.2608\n",
      "Epoch 1,Step [5631/12942],4774.049034357071s, Loss:2.2312\n",
      "Epoch 1,Step [5632/12942],4778.868842840195s, Loss:2.3284\n",
      "Epoch 1,Step [5633/12942],4783.947743654251s, Loss:2.0406\n",
      "Epoch 1,Step [5634/12942],4790.896080732346s, Loss:2.0345\n",
      "Epoch 1,Step [5635/12942],4824.575241804123s, Loss:2.2871\n",
      "Epoch 1,Step [5636/12942],4858.180590867996s, Loss:2.0314\n",
      "Epoch 1,Step [5637/12942],4891.078580379486s, Loss:1.8698\n",
      "Epoch 1,Step [5638/12942],4923.226073980331s, Loss:2.3764\n",
      "Epoch 1,Step [5639/12942],4954.260594129562s, Loss:2.1591\n",
      "Epoch 1,Step [5640/12942],4984.803968906403s, Loss:2.2258\n",
      "Epoch 1,Step [5641/12942],5023.62766289711s, Loss:2.2404\n",
      "Epoch 1,Step [5642/12942],5055.19934630394s, Loss:2.0543\n",
      "Epoch 1,Step [5643/12942],5060.405079841614s, Loss:2.2703\n",
      "Epoch 1,Step [5644/12942],5065.233753681183s, Loss:2.2702\n",
      "Epoch 1,Step [5645/12942],5070.346660614014s, Loss:1.9574\n",
      "Epoch 1,Step [5646/12942],5075.494988918304s, Loss:2.2776\n",
      "Epoch 1,Step [5647/12942],5080.523641586304s, Loss:2.5698\n",
      "Epoch 1,Step [5648/12942],5085.695648908615s, Loss:2.2410\n",
      "Epoch 1,Step [5649/12942],5094.138296842575s, Loss:2.3532\n",
      "Epoch 1,Step [5650/12942],5122.865988492966s, Loss:1.9434\n",
      "Epoch 1,Step [5651/12942],5153.896117687225s, Loss:2.0764\n",
      "Epoch 1,Step [5652/12942],5194.228391885757s, Loss:1.9715\n",
      "Epoch 1,Step [5653/12942],5225.805419206619s, Loss:2.3902\n",
      "Epoch 1,Step [5654/12942],5258.637465000153s, Loss:2.1593\n",
      "Epoch 1,Step [5655/12942],5290.209007501602s, Loss:2.0636\n",
      "Epoch 1,Step [5656/12942],5323.553105831146s, Loss:2.4927\n",
      "Epoch 1,Step [5657/12942],5356.185931682587s, Loss:2.5229\n",
      "Epoch 1,Step [5658/12942],5362.171418428421s, Loss:2.2263\n",
      "Epoch 1,Step [5659/12942],5367.4032509326935s, Loss:2.4709\n",
      "Epoch 1,Step [5660/12942],5372.5807292461395s, Loss:2.1653\n",
      "Epoch 1,Step [5661/12942],5377.78334569931s, Loss:1.9674\n",
      "Epoch 1,Step [5662/12942],5382.837656736374s, Loss:2.2706\n",
      "Epoch 1,Step [5663/12942],5388.594268798828s, Loss:2.2742\n",
      "Epoch 1,Step [5664/12942],5406.506403207779s, Loss:2.1170\n",
      "Epoch 1,Step [5665/12942],5436.882567167282s, Loss:2.3172\n",
      "Epoch 1,Step [5666/12942],5469.682254552841s, Loss:2.2644\n",
      "Epoch 1,Step [5667/12942],5499.480748414993s, Loss:2.1160\n",
      "Epoch 1,Step [5668/12942],5532.567111492157s, Loss:2.5425\n",
      "Epoch 1,Step [5669/12942],5571.2747485637665s, Loss:2.6663\n",
      "Epoch 1,Step [5670/12942],5604.162269353867s, Loss:2.2521\n",
      "Epoch 1,Step [5671/12942],5637.568356513977s, Loss:2.2406\n",
      "Epoch 1,Step [5672/12942],5657.807792901993s, Loss:2.3439\n",
      "Epoch 1,Step [5673/12942],5662.865429639816s, Loss:2.1767\n",
      "Epoch 1,Step [5674/12942],5668.033539533615s, Loss:2.1826\n",
      "Epoch 1,Step [5675/12942],5673.2607452869415s, Loss:2.2862\n",
      "Epoch 1,Step [5676/12942],5678.144903659821s, Loss:2.0031\n",
      "Epoch 1,Step [5677/12942],5683.290080785751s, Loss:2.4558\n",
      "Epoch 1,Step [5678/12942],5689.207785606384s, Loss:2.2826\n",
      "Epoch 1,Step [5679/12942],5710.426266908646s, Loss:2.3050\n",
      "Epoch 1,Step [5680/12942],5747.946437597275s, Loss:2.4810\n",
      "Epoch 1,Step [5681/12942],5778.455729722977s, Loss:2.2343\n",
      "Epoch 1,Step [5682/12942],5811.354128837585s, Loss:2.0573\n",
      "Epoch 1,Step [5683/12942],5843.275870800018s, Loss:2.1019\n",
      "Epoch 1,Step [5684/12942],5876.126952648163s, Loss:2.0186\n",
      "Epoch 1,Step [5685/12942],5917.043335676193s, Loss:2.3192\n",
      "Epoch 1,Step [5686/12942],5949.582509756088s, Loss:1.9793\n",
      "Epoch 1,Step [5687/12942],5959.927883148193s, Loss:1.8723\n",
      "Epoch 1,Step [5688/12942],5965.24121761322s, Loss:2.3391\n",
      "Epoch 1,Step [5689/12942],5970.066806077957s, Loss:2.1743\n",
      "Epoch 1,Step [5690/12942],5975.125645399094s, Loss:1.9216\n",
      "Epoch 1,Step [5691/12942],5980.285898685455s, Loss:2.1935\n",
      "Epoch 1,Step [5692/12942],5985.494183063507s, Loss:2.4198\n",
      "Epoch 1,Step [5693/12942],5994.5760242938995s, Loss:2.5077\n",
      "Epoch 1,Step [5694/12942],6023.029924154282s, Loss:2.2715\n",
      "Epoch 1,Step [5695/12942],6055.331959962845s, Loss:2.1822\n",
      "Epoch 1,Step [5696/12942],6096.47908616066s, Loss:2.3049\n",
      "Epoch 1,Step [5697/12942],6127.732925415039s, Loss:2.9167\n",
      "Epoch 1,Step [5698/12942],6158.199725627899s, Loss:2.4415\n",
      "Epoch 1,Step [5699/12942],6190.011761188507s, Loss:2.2190\n",
      "Epoch 1,Step [5700/12942],6220.335232019424s, Loss:1.9843\n",
      "Epoch 1,Step [5700/12942],6220.336452007294s, Loss:1.9843\n",
      "\n",
      "Epoch 1,Step [5701/12942],34.66763997077942s, Loss:2.3869\n",
      "Epoch 1,Step [5702/12942],40.95342779159546s, Loss:2.7477\n",
      "Epoch 1,Step [5703/12942],46.1950957775116s, Loss:2.1799\n",
      "Epoch 1,Step [5704/12942],51.43174481391907s, Loss:2.1724\n",
      "Epoch 1,Step [5705/12942],56.6984441280365s, Loss:2.1718\n",
      "Epoch 1,Step [5706/12942],61.92932200431824s, Loss:2.5341\n",
      "Epoch 1,Step [5707/12942],67.87514042854309s, Loss:2.1002\n",
      "Epoch 1,Step [5708/12942],85.43488097190857s, Loss:2.3454\n",
      "Epoch 1,Step [5709/12942],115.3291027545929s, Loss:1.9830\n",
      "Epoch 1,Step [5710/12942],146.5839273929596s, Loss:2.9636\n",
      "Epoch 1,Step [5711/12942],177.66303777694702s, Loss:2.2250\n",
      "Epoch 1,Step [5712/12942],217.02026462554932s, Loss:3.0173\n",
      "Epoch 1,Step [5713/12942],251.2447383403778s, Loss:2.5704\n",
      "Epoch 1,Step [5714/12942],284.43281388282776s, Loss:1.9482\n",
      "Epoch 1,Step [5715/12942],316.1642496585846s, Loss:2.2178\n",
      "Epoch 1,Step [5716/12942],336.6047053337097s, Loss:2.5481\n",
      "Epoch 1,Step [5717/12942],341.7263650894165s, Loss:2.0040\n",
      "Epoch 1,Step [5718/12942],346.93616676330566s, Loss:2.0901\n",
      "Epoch 1,Step [5719/12942],352.1928939819336s, Loss:2.1404\n",
      "Epoch 1,Step [5720/12942],357.35144543647766s, Loss:2.3788\n",
      "Epoch 1,Step [5721/12942],362.5930690765381s, Loss:2.1168\n",
      "Epoch 1,Step [5722/12942],369.23457384109497s, Loss:2.1833\n",
      "Epoch 1,Step [5723/12942],396.50153160095215s, Loss:2.4919\n",
      "Epoch 1,Step [5724/12942],431.73710536956787s, Loss:2.3356\n",
      "Epoch 1,Step [5725/12942],459.9498715400696s, Loss:2.6196\n",
      "Epoch 1,Step [5726/12942],493.1494860649109s, Loss:1.9954\n",
      "Epoch 1,Step [5727/12942],523.2634131908417s, Loss:2.3886\n",
      "Epoch 1,Step [5728/12942],554.942417383194s, Loss:2.4332\n",
      "Epoch 1,Step [5729/12942],594.6972720623016s, Loss:2.1491\n",
      "Epoch 1,Step [5730/12942],626.209466457367s, Loss:2.1464\n",
      "Epoch 1,Step [5731/12942],638.6206684112549s, Loss:2.0829\n",
      "Epoch 1,Step [5732/12942],643.8502354621887s, Loss:2.1590\n",
      "Epoch 1,Step [5733/12942],648.696729183197s, Loss:2.1835\n",
      "Epoch 1,Step [5734/12942],653.8812851905823s, Loss:2.2726\n",
      "Epoch 1,Step [5735/12942],659.0243043899536s, Loss:2.3199\n",
      "Epoch 1,Step [5736/12942],664.1980075836182s, Loss:2.3636\n",
      "Epoch 1,Step [5737/12942],672.6530804634094s, Loss:2.1298\n",
      "Epoch 1,Step [5738/12942],700.1084082126617s, Loss:2.1362\n",
      "Epoch 1,Step [5739/12942],733.0599586963654s, Loss:1.9938\n",
      "Epoch 1,Step [5740/12942],773.6595032215118s, Loss:2.0867\n",
      "Epoch 1,Step [5741/12942],807.5777456760406s, Loss:2.4705\n",
      "Epoch 1,Step [5742/12942],839.0554189682007s, Loss:2.3134\n",
      "Epoch 1,Step [5743/12942],870.2076811790466s, Loss:2.3611\n",
      "Epoch 1,Step [5744/12942],902.7864863872528s, Loss:2.5623\n",
      "Epoch 1,Step [5745/12942],935.4431710243225s, Loss:2.4044\n",
      "Epoch 1,Step [5746/12942],941.2335495948792s, Loss:2.2819\n",
      "Epoch 1,Step [5747/12942],946.4421348571777s, Loss:2.1057\n",
      "Epoch 1,Step [5748/12942],951.6596984863281s, Loss:1.9305\n",
      "Epoch 1,Step [5749/12942],956.8830103874207s, Loss:2.1150\n",
      "Epoch 1,Step [5750/12942],962.1670715808868s, Loss:2.2693\n",
      "Epoch 1,Step [5751/12942],968.4343645572662s, Loss:2.4888\n",
      "Epoch 1,Step [5752/12942],988.4902210235596s, Loss:2.4550\n",
      "Epoch 1,Step [5753/12942],1021.1276440620422s, Loss:2.3186\n",
      "Epoch 1,Step [5754/12942],1052.3597514629364s, Loss:2.2077\n",
      "Epoch 1,Step [5755/12942],1084.6186509132385s, Loss:2.0901\n",
      "Epoch 1,Step [5756/12942],1124.1621460914612s, Loss:2.2808\n",
      "Epoch 1,Step [5757/12942],1157.292237520218s, Loss:2.1895\n",
      "Epoch 1,Step [5758/12942],1187.7001974582672s, Loss:1.9268\n",
      "Epoch 1,Step [5759/12942],1220.2412259578705s, Loss:2.3805\n",
      "Epoch 1,Step [5760/12942],1237.6639001369476s, Loss:2.0707\n",
      "Epoch 1,Step [5761/12942],1242.8342657089233s, Loss:2.4159\n",
      "Epoch 1,Step [5762/12942],1248.1766953468323s, Loss:2.1554\n",
      "Epoch 1,Step [5763/12942],1253.4085521697998s, Loss:2.1113\n",
      "Epoch 1,Step [5764/12942],1258.6308853626251s, Loss:2.1116\n",
      "Epoch 1,Step [5765/12942],1263.8270671367645s, Loss:2.3404\n",
      "Epoch 1,Step [5766/12942],1271.471144914627s, Loss:2.1937\n",
      "Epoch 1,Step [5767/12942],1306.7607009410858s, Loss:2.2886\n",
      "Epoch 1,Step [5768/12942],1338.659057855606s, Loss:2.4306\n",
      "Epoch 1,Step [5769/12942],1369.8405666351318s, Loss:2.1517\n",
      "Epoch 1,Step [5770/12942],1401.3607017993927s, Loss:1.9074\n",
      "Epoch 1,Step [5771/12942],1433.9113187789917s, Loss:2.4006\n",
      "Epoch 1,Step [5772/12942],1468.660020828247s, Loss:2.9578\n",
      "Epoch 1,Step [5773/12942],1507.1933748722076s, Loss:2.5757\n",
      "Epoch 1,Step [5774/12942],1535.414731502533s, Loss:1.9244\n",
      "Epoch 1,Step [5775/12942],1540.3626804351807s, Loss:2.2104\n",
      "Epoch 1,Step [5776/12942],1545.593423128128s, Loss:2.3771\n",
      "Epoch 1,Step [5777/12942],1550.9089999198914s, Loss:2.3327\n",
      "Epoch 1,Step [5778/12942],1556.0911676883698s, Loss:2.1472\n",
      "Epoch 1,Step [5779/12942],1561.2792127132416s, Loss:2.5375\n",
      "Epoch 1,Step [5780/12942],1567.1404852867126s, Loss:2.2292\n",
      "Epoch 1,Step [5781/12942],1581.2742841243744s, Loss:2.1807\n",
      "Epoch 1,Step [5782/12942],1612.8983809947968s, Loss:2.4444\n",
      "Epoch 1,Step [5783/12942],1643.9952754974365s, Loss:2.2864\n",
      "Epoch 1,Step [5784/12942],1684.0084795951843s, Loss:1.7881\n",
      "Epoch 1,Step [5785/12942],1715.9552121162415s, Loss:2.2132\n",
      "Epoch 1,Step [5786/12942],1747.7766139507294s, Loss:2.1663\n",
      "Epoch 1,Step [5787/12942],1781.2628331184387s, Loss:2.3127\n",
      "Epoch 1,Step [5788/12942],1812.4303450584412s, Loss:2.0412\n",
      "Epoch 1,Step [5789/12942],1838.0054569244385s, Loss:2.0927\n",
      "Epoch 1,Step [5790/12942],1843.2568156719208s, Loss:2.0597\n",
      "Epoch 1,Step [5791/12942],1848.184564590454s, Loss:2.0923\n",
      "Epoch 1,Step [5792/12942],1853.304200410843s, Loss:1.9858\n",
      "Epoch 1,Step [5793/12942],1858.476853609085s, Loss:2.4961\n",
      "Epoch 1,Step [5794/12942],1863.5668256282806s, Loss:2.0932\n",
      "Epoch 1,Step [5795/12942],1871.296154975891s, Loss:2.1571\n",
      "Epoch 1,Step [5796/12942],1897.0481293201447s, Loss:2.6063\n",
      "Epoch 1,Step [5797/12942],1928.1028845310211s, Loss:2.2728\n",
      "Epoch 1,Step [5798/12942],1960.225261926651s, Loss:2.1923\n",
      "Epoch 1,Step [5799/12942],1992.8572912216187s, Loss:2.9986\n",
      "Epoch 1,Step [5800/12942],2033.8073012828827s, Loss:2.4250\n",
      "Epoch 1,Step [5801/12942],2065.5260775089264s, Loss:2.1875\n",
      "Epoch 1,Step [5802/12942],2095.77410364151s, Loss:2.6199\n",
      "Epoch 1,Step [5803/12942],2128.499052286148s, Loss:2.1656\n",
      "Epoch 1,Step [5804/12942],2138.7421247959137s, Loss:2.1528\n",
      "Epoch 1,Step [5805/12942],2143.974593639374s, Loss:2.2650\n",
      "Epoch 1,Step [5806/12942],2149.0568673610687s, Loss:2.1887\n",
      "Epoch 1,Step [5807/12942],2153.8857820034027s, Loss:1.9324\n",
      "Epoch 1,Step [5808/12942],2158.901604413986s, Loss:2.1578\n",
      "Epoch 1,Step [5809/12942],2163.8492991924286s, Loss:2.4902\n",
      "Epoch 1,Step [5810/12942],2171.337438583374s, Loss:2.5499\n",
      "Epoch 1,Step [5811/12942],2203.6754112243652s, Loss:1.9868\n",
      "Epoch 1,Step [5812/12942],2236.695766687393s, Loss:2.1705\n",
      "Epoch 1,Step [5813/12942],2269.999911546707s, Loss:2.1490\n",
      "Epoch 1,Step [5814/12942],2301.588913679123s, Loss:2.6199\n",
      "Epoch 1,Step [5815/12942],2334.056580543518s, Loss:2.6100\n",
      "Epoch 1,Step [5816/12942],2363.949172973633s, Loss:2.1200\n",
      "Epoch 1,Step [5817/12942],2403.8362522125244s, Loss:2.2898\n",
      "Epoch 1,Step [5818/12942],2435.01953792572s, Loss:2.0950\n",
      "Epoch 1,Step [5819/12942],2440.36026930809s, Loss:2.7592\n",
      "Epoch 1,Step [5820/12942],2445.6579937934875s, Loss:2.6186\n",
      "Epoch 1,Step [5821/12942],2450.92630815506s, Loss:2.5393\n",
      "Epoch 1,Step [5822/12942],2455.9566366672516s, Loss:2.5372\n",
      "Epoch 1,Step [5823/12942],2461.1746366024017s, Loss:2.1433\n",
      "Epoch 1,Step [5824/12942],2466.6356308460236s, Loss:2.0192\n",
      "Epoch 1,Step [5825/12942],2480.416933774948s, Loss:1.9381\n",
      "Epoch 1,Step [5826/12942],2512.6502137184143s, Loss:2.4498\n",
      "Epoch 1,Step [5827/12942],2545.4385557174683s, Loss:2.0111\n",
      "Epoch 1,Step [5828/12942],2583.6888778209686s, Loss:2.1707\n",
      "Epoch 1,Step [5829/12942],2616.26105427742s, Loss:2.4912\n",
      "Epoch 1,Step [5830/12942],2648.9067854881287s, Loss:2.0840\n",
      "Epoch 1,Step [5831/12942],2682.9130716323853s, Loss:2.2145\n",
      "Epoch 1,Step [5832/12942],2713.715598344803s, Loss:2.1461\n",
      "Epoch 1,Step [5833/12942],2738.2081837654114s, Loss:2.0158\n",
      "Epoch 1,Step [5834/12942],2743.4025762081146s, Loss:2.3485\n",
      "Epoch 1,Step [5835/12942],2748.6608641147614s, Loss:2.1316\n",
      "Epoch 1,Step [5836/12942],2753.8506672382355s, Loss:2.1336\n",
      "Epoch 1,Step [5837/12942],2758.965427160263s, Loss:2.2562\n",
      "Epoch 1,Step [5838/12942],2764.2542383670807s, Loss:2.2538\n",
      "Epoch 1,Step [5839/12942],2772.8604593276978s, Loss:1.9976\n",
      "Epoch 1,Step [5840/12942],2798.6510813236237s, Loss:2.3028\n",
      "Epoch 1,Step [5841/12942],2830.7928223609924s, Loss:2.0002\n",
      "Epoch 1,Step [5842/12942],2861.100481748581s, Loss:2.4234\n",
      "Epoch 1,Step [5843/12942],2892.9722125530243s, Loss:1.7849\n",
      "Epoch 1,Step [5844/12942],2933.614491701126s, Loss:3.0811\n",
      "Epoch 1,Step [5845/12942],2964.2846236228943s, Loss:2.0463\n",
      "Epoch 1,Step [5846/12942],2994.5809376239777s, Loss:2.1169\n",
      "Epoch 1,Step [5847/12942],3025.2856431007385s, Loss:2.0210\n",
      "Epoch 1,Step [5848/12942],3038.470506668091s, Loss:2.4415\n",
      "Epoch 1,Step [5849/12942],3043.484673023224s, Loss:2.4500\n",
      "Epoch 1,Step [5850/12942],3048.3045103549957s, Loss:2.0770\n",
      "Epoch 1,Step [5851/12942],3053.400105237961s, Loss:1.8658\n",
      "Epoch 1,Step [5852/12942],3058.667417526245s, Loss:2.5538\n",
      "Epoch 1,Step [5853/12942],3063.9007201194763s, Loss:2.3599\n",
      "Epoch 1,Step [5854/12942],3071.9204530715942s, Loss:2.2410\n",
      "Epoch 1,Step [5855/12942],3107.7989377975464s, Loss:2.5706\n",
      "Epoch 1,Step [5856/12942],3140.2411291599274s, Loss:2.1987\n",
      "Epoch 1,Step [5857/12942],3173.9221510887146s, Loss:2.3812\n",
      "Epoch 1,Step [5858/12942],3204.9023973941803s, Loss:1.9632\n",
      "Epoch 1,Step [5859/12942],3233.8343074321747s, Loss:2.4232\n",
      "Epoch 1,Step [5860/12942],3267.5614359378815s, Loss:2.6024\n",
      "Epoch 1,Step [5861/12942],3304.7963609695435s, Loss:2.0118\n",
      "Epoch 1,Step [5862/12942],3335.480791568756s, Loss:2.2754\n",
      "Epoch 1,Step [5863/12942],3340.625787258148s, Loss:2.0691\n",
      "Epoch 1,Step [5864/12942],3345.47780585289s, Loss:2.0006\n",
      "Epoch 1,Step [5865/12942],3350.7032749652863s, Loss:3.3098\n",
      "Epoch 1,Step [5866/12942],3355.6049947738647s, Loss:2.9440\n",
      "Epoch 1,Step [5867/12942],3360.79767370224s, Loss:2.0567\n",
      "Epoch 1,Step [5868/12942],3365.850542783737s, Loss:2.2056\n",
      "Epoch 1,Step [5869/12942],3376.165910243988s, Loss:2.4153\n",
      "Epoch 1,Step [5870/12942],3408.218373775482s, Loss:2.1977\n",
      "Epoch 1,Step [5871/12942],3441.0550112724304s, Loss:2.1571\n",
      "Epoch 1,Step [5872/12942],3482.195900917053s, Loss:2.4279\n",
      "Epoch 1,Step [5873/12942],3514.423152923584s, Loss:1.8725\n",
      "Epoch 1,Step [5874/12942],3544.677767753601s, Loss:1.9980\n",
      "Epoch 1,Step [5875/12942],3575.7707645893097s, Loss:2.3649\n",
      "Epoch 1,Step [5876/12942],3607.0006725788116s, Loss:2.1991\n",
      "Epoch 1,Step [5877/12942],3636.511920452118s, Loss:2.1562\n",
      "Epoch 1,Step [5878/12942],3642.239013195038s, Loss:2.1806\n",
      "Epoch 1,Step [5879/12942],3647.4767265319824s, Loss:2.8112\n",
      "Epoch 1,Step [5880/12942],3652.621024608612s, Loss:2.1604\n",
      "Epoch 1,Step [5881/12942],3657.817806482315s, Loss:2.1014\n",
      "Epoch 1,Step [5882/12942],3663.068606376648s, Loss:1.9905\n",
      "Epoch 1,Step [5883/12942],3669.6622631549835s, Loss:1.8432\n",
      "Epoch 1,Step [5884/12942],3693.087208509445s, Loss:2.3792\n",
      "Epoch 1,Step [5885/12942],3726.7759640216827s, Loss:2.5296\n",
      "Epoch 1,Step [5886/12942],3758.073484182358s, Loss:2.8578\n",
      "Epoch 1,Step [5887/12942],3789.5106048583984s, Loss:2.2492\n",
      "Epoch 1,Step [5888/12942],3830.752490758896s, Loss:2.4172\n",
      "Epoch 1,Step [5889/12942],3861.9147431850433s, Loss:2.1195\n",
      "Epoch 1,Step [5890/12942],3894.624706506729s, Loss:1.9664\n",
      "Epoch 1,Step [5891/12942],3926.119364261627s, Loss:2.6374\n",
      "Epoch 1,Step [5892/12942],3938.2947237491608s, Loss:1.9948\n",
      "Epoch 1,Step [5893/12942],3943.551705121994s, Loss:2.2111\n",
      "Epoch 1,Step [5894/12942],3948.783511161804s, Loss:1.9102\n",
      "Epoch 1,Step [5895/12942],3953.9941744804382s, Loss:2.1915\n",
      "Epoch 1,Step [5896/12942],3959.194315433502s, Loss:2.4523\n",
      "Epoch 1,Step [5897/12942],3964.0898039340973s, Loss:2.1618\n",
      "Epoch 1,Step [5898/12942],3972.2518973350525s, Loss:2.1921\n",
      "Epoch 1,Step [5899/12942],4009.2650468349457s, Loss:2.1991\n",
      "Epoch 1,Step [5900/12942],4040.4300649166107s, Loss:2.2902\n",
      "Epoch 1,Step [5901/12942],4068.47452712059s, Loss:2.1412\n",
      "Epoch 1,Step [5902/12942],4100.272723197937s, Loss:2.4154\n",
      "Epoch 1,Step [5903/12942],4132.312433958054s, Loss:2.1428\n",
      "Epoch 1,Step [5904/12942],4166.443420886993s, Loss:1.9936\n",
      "Epoch 1,Step [5905/12942],4204.784014225006s, Loss:2.2468\n",
      "Epoch 1,Step [5906/12942],4235.253423690796s, Loss:2.3642\n",
      "Epoch 1,Step [5907/12942],4240.094532728195s, Loss:2.3463\n",
      "Epoch 1,Step [5908/12942],4245.359366416931s, Loss:2.2965\n",
      "Epoch 1,Step [5909/12942],4250.312528371811s, Loss:2.4989\n",
      "Epoch 1,Step [5910/12942],4255.506074428558s, Loss:2.1490\n",
      "Epoch 1,Step [5911/12942],4260.703256368637s, Loss:2.3993\n",
      "Epoch 1,Step [5912/12942],4266.2412276268005s, Loss:2.2949\n",
      "Epoch 1,Step [5913/12942],4278.728233337402s, Loss:2.1979\n",
      "Epoch 1,Step [5914/12942],4308.175455093384s, Loss:2.2587\n",
      "Epoch 1,Step [5915/12942],4337.983434677124s, Loss:2.2572\n",
      "Epoch 1,Step [5916/12942],4378.166552305222s, Loss:2.3298\n",
      "Epoch 1,Step [5917/12942],4409.130627632141s, Loss:2.3606\n",
      "Epoch 1,Step [5918/12942],4441.108559131622s, Loss:2.5387\n",
      "Epoch 1,Step [5919/12942],4473.488160610199s, Loss:2.2624\n",
      "Epoch 1,Step [5920/12942],4504.9766364097595s, Loss:2.2861\n",
      "Epoch 1,Step [5921/12942],4536.188469409943s, Loss:2.4353\n",
      "Epoch 1,Step [5922/12942],4541.941893577576s, Loss:2.3579\n",
      "Epoch 1,Step [5923/12942],4547.174635410309s, Loss:2.1071\n",
      "Epoch 1,Step [5924/12942],4552.352199792862s, Loss:2.7739\n",
      "Epoch 1,Step [5925/12942],4557.446223258972s, Loss:2.2968\n",
      "Epoch 1,Step [5926/12942],4562.641528606415s, Loss:2.1473\n",
      "Epoch 1,Step [5927/12942],4569.052144527435s, Loss:2.1097\n",
      "Epoch 1,Step [5928/12942],4587.209440946579s, Loss:2.2242\n",
      "Epoch 1,Step [5929/12942],4618.69181394577s, Loss:2.0358\n",
      "Epoch 1,Step [5930/12942],4651.968116521835s, Loss:2.1903\n",
      "Epoch 1,Step [5931/12942],4682.941462993622s, Loss:2.4903\n",
      "Epoch 1,Step [5932/12942],4722.047671794891s, Loss:2.5009\n",
      "Epoch 1,Step [5933/12942],4754.762610912323s, Loss:2.1650\n",
      "Epoch 1,Step [5934/12942],4784.005256414413s, Loss:2.4964\n",
      "Epoch 1,Step [5935/12942],4816.480731487274s, Loss:2.2417\n",
      "Epoch 1,Step [5936/12942],4837.058836936951s, Loss:2.1469\n",
      "Epoch 1,Step [5937/12942],4842.023442745209s, Loss:2.4527\n",
      "Epoch 1,Step [5938/12942],4847.221476078033s, Loss:2.4084\n",
      "Epoch 1,Step [5939/12942],4852.490019321442s, Loss:2.5429\n",
      "Epoch 1,Step [5940/12942],4857.6868414878845s, Loss:2.1446\n",
      "Epoch 1,Step [5941/12942],4862.8576567173s, Loss:2.1308\n",
      "Epoch 1,Step [5942/12942],4869.463512420654s, Loss:2.3639\n",
      "Epoch 1,Step [5943/12942],4898.439729690552s, Loss:2.1705\n",
      "Epoch 1,Step [5944/12942],4932.58206319809s, Loss:2.1791\n",
      "Epoch 1,Step [5945/12942],4963.138656616211s, Loss:2.7270\n",
      "Epoch 1,Step [5946/12942],4994.910015821457s, Loss:2.2264\n",
      "Epoch 1,Step [5947/12942],5027.111078739166s, Loss:2.2556\n",
      "Epoch 1,Step [5948/12942],5059.09273815155s, Loss:1.9183\n",
      "Epoch 1,Step [5949/12942],5099.425663948059s, Loss:2.0121\n",
      "Epoch 1,Step [5950/12942],5132.500323534012s, Loss:2.3481\n",
      "Epoch 1,Step [5951/12942],5139.572246551514s, Loss:2.3768\n",
      "Epoch 1,Step [5952/12942],5144.852878808975s, Loss:2.0556\n",
      "Epoch 1,Step [5953/12942],5149.845922231674s, Loss:2.1358\n",
      "Epoch 1,Step [5954/12942],5155.093881607056s, Loss:2.2475\n",
      "Epoch 1,Step [5955/12942],5160.2977085113525s, Loss:2.2076\n",
      "Epoch 1,Step [5956/12942],5165.1617686748505s, Loss:1.9394\n",
      "Epoch 1,Step [5957/12942],5174.494517326355s, Loss:2.1278\n",
      "Epoch 1,Step [5958/12942],5205.61891913414s, Loss:2.1542\n",
      "Epoch 1,Step [5959/12942],5236.677610874176s, Loss:1.9408\n",
      "Epoch 1,Step [5960/12942],5277.371906042099s, Loss:2.1104\n",
      "Epoch 1,Step [5961/12942],5309.283740758896s, Loss:2.2035\n",
      "Epoch 1,Step [5962/12942],5343.307658195496s, Loss:2.2833\n",
      "Epoch 1,Step [5963/12942],5373.154726982117s, Loss:2.2258\n",
      "Epoch 1,Step [5964/12942],5404.659295558929s, Loss:2.1997\n",
      "Epoch 1,Step [5965/12942],5436.096965551376s, Loss:1.7684\n",
      "Epoch 1,Step [5966/12942],5441.564866781235s, Loss:2.0036\n",
      "Epoch 1,Step [5967/12942],5446.799068212509s, Loss:2.2287\n",
      "Epoch 1,Step [5968/12942],5452.06912779808s, Loss:2.2401\n",
      "Epoch 1,Step [5969/12942],5457.324902057648s, Loss:1.8869\n",
      "Epoch 1,Step [5970/12942],5462.615474462509s, Loss:2.1061\n",
      "Epoch 1,Step [5971/12942],5469.170637845993s, Loss:2.1052\n",
      "Epoch 1,Step [5972/12942],5490.670487642288s, Loss:2.1123\n",
      "Epoch 1,Step [5973/12942],5522.644022464752s, Loss:2.2474\n",
      "Epoch 1,Step [5974/12942],5553.757838964462s, Loss:2.4188\n",
      "Epoch 1,Step [5975/12942],5587.055267333984s, Loss:2.1659\n",
      "Epoch 1,Step [5976/12942],5628.11927986145s, Loss:2.0230\n",
      "Epoch 1,Step [5977/12942],5660.9773025512695s, Loss:2.1621\n",
      "Epoch 1,Step [5978/12942],5693.648564577103s, Loss:2.1597\n",
      "Epoch 1,Step [5979/12942],5726.904389858246s, Loss:2.2704\n",
      "Epoch 1,Step [5980/12942],5738.461988925934s, Loss:2.2594\n",
      "Epoch 1,Step [5981/12942],5743.700622320175s, Loss:2.3711\n",
      "Epoch 1,Step [5982/12942],5748.601642608643s, Loss:2.3160\n",
      "Epoch 1,Step [5983/12942],5753.641325712204s, Loss:2.3809\n",
      "Epoch 1,Step [5984/12942],5758.9057059288025s, Loss:2.8359\n",
      "Epoch 1,Step [5985/12942],5764.068513393402s, Loss:2.0804\n",
      "Epoch 1,Step [5986/12942],5772.116597175598s, Loss:2.1624\n",
      "Epoch 1,Step [5987/12942],5807.641241312027s, Loss:2.0168\n",
      "Epoch 1,Step [5988/12942],5840.3455555438995s, Loss:2.2068\n",
      "Epoch 1,Step [5989/12942],5871.442471265793s, Loss:2.5712\n",
      "Epoch 1,Step [5990/12942],5902.590126752853s, Loss:2.4620\n",
      "Epoch 1,Step [5991/12942],5933.991948127747s, Loss:2.3988\n",
      "Epoch 1,Step [5992/12942],5963.726315021515s, Loss:2.1163\n",
      "Epoch 1,Step [5993/12942],6002.181763648987s, Loss:2.2524\n",
      "Epoch 1,Step [5994/12942],6034.74551320076s, Loss:2.4360\n",
      "Epoch 1,Step [5995/12942],6040.080974817276s, Loss:2.1869\n",
      "Epoch 1,Step [5996/12942],6044.965087175369s, Loss:2.7936\n",
      "Epoch 1,Step [5997/12942],6050.184291601181s, Loss:2.1529\n",
      "Epoch 1,Step [5998/12942],6055.462010145187s, Loss:2.3644\n",
      "Epoch 1,Step [5999/12942],6060.682932138443s, Loss:2.2716\n",
      "Epoch 1,Step [6000/12942],6066.179237365723s, Loss:2.1219\n",
      "Epoch 1,Step [6000/12942],6066.189678907394s, Loss:2.1219\n",
      "\n",
      "Epoch 1,Step [6001/12942],13.651131868362427s, Loss:2.0154\n",
      "Epoch 1,Step [6002/12942],45.78161120414734s, Loss:2.1028\n",
      "Epoch 1,Step [6003/12942],78.60076713562012s, Loss:1.9979\n",
      "Epoch 1,Step [6004/12942],116.3150703907013s, Loss:2.3548\n",
      "Epoch 1,Step [6005/12942],148.81406140327454s, Loss:2.1484\n",
      "Epoch 1,Step [6006/12942],178.75815987586975s, Loss:2.2334\n",
      "Epoch 1,Step [6007/12942],212.00040817260742s, Loss:2.0982\n",
      "Epoch 1,Step [6008/12942],243.19973754882812s, Loss:2.3652\n",
      "Epoch 1,Step [6009/12942],270.95308017730713s, Loss:2.6133\n",
      "Epoch 1,Step [6010/12942],275.97914695739746s, Loss:2.8815\n",
      "Epoch 1,Step [6011/12942],281.1574606895447s, Loss:2.0867\n",
      "Epoch 1,Step [6012/12942],286.40884375572205s, Loss:1.9727\n",
      "Epoch 1,Step [6013/12942],291.6567118167877s, Loss:2.0994\n",
      "Epoch 1,Step [6014/12942],296.7488977909088s, Loss:2.2294\n",
      "Epoch 1,Step [6015/12942],303.41245794296265s, Loss:2.5925\n",
      "Epoch 1,Step [6016/12942],327.8590247631073s, Loss:2.2305\n",
      "Epoch 1,Step [6017/12942],361.9921591281891s, Loss:2.3350\n",
      "Epoch 1,Step [6018/12942],393.01415371894836s, Loss:2.1576\n",
      "Epoch 1,Step [6019/12942],425.08184576034546s, Loss:2.1228\n",
      "Epoch 1,Step [6020/12942],465.3136200904846s, Loss:2.2994\n",
      "Epoch 1,Step [6021/12942],495.71660137176514s, Loss:2.1643\n",
      "Epoch 1,Step [6022/12942],527.5588133335114s, Loss:1.8036\n",
      "Epoch 1,Step [6023/12942],559.8481760025024s, Loss:3.1303\n",
      "Epoch 1,Step [6024/12942],572.1861870288849s, Loss:2.0761\n",
      "Epoch 1,Step [6025/12942],577.3518526554108s, Loss:2.4080\n",
      "Epoch 1,Step [6026/12942],582.5804159641266s, Loss:2.0875\n",
      "Epoch 1,Step [6027/12942],587.8557500839233s, Loss:2.2000\n",
      "Epoch 1,Step [6028/12942],593.0484836101532s, Loss:2.3543\n",
      "Epoch 1,Step [6029/12942],597.9563636779785s, Loss:2.5031\n",
      "Epoch 1,Step [6030/12942],606.4422745704651s, Loss:2.3281\n",
      "Epoch 1,Step [6031/12942],641.7243099212646s, Loss:2.1868\n",
      "Epoch 1,Step [6032/12942],673.5419523715973s, Loss:2.0639\n",
      "Epoch 1,Step [6033/12942],705.1547980308533s, Loss:2.3698\n",
      "Epoch 1,Step [6034/12942],737.7194671630859s, Loss:2.2222\n",
      "Epoch 1,Step [6035/12942],771.1843280792236s, Loss:2.4302\n",
      "Epoch 1,Step [6036/12942],807.0308494567871s, Loss:2.4126\n",
      "Epoch 1,Step [6037/12942],843.8390972614288s, Loss:1.8413\n",
      "Epoch 1,Step [6038/12942],869.6356542110443s, Loss:2.3185\n",
      "Epoch 1,Step [6039/12942],874.832505941391s, Loss:2.1075\n",
      "Epoch 1,Step [6040/12942],879.8571400642395s, Loss:2.1443\n",
      "Epoch 1,Step [6041/12942],885.1006891727448s, Loss:2.3257\n",
      "Epoch 1,Step [6042/12942],890.129700422287s, Loss:2.1798\n",
      "Epoch 1,Step [6043/12942],895.4266858100891s, Loss:2.2794\n",
      "Epoch 1,Step [6044/12942],901.5203516483307s, Loss:2.2266\n",
      "Epoch 1,Step [6045/12942],919.3737344741821s, Loss:2.4889\n",
      "Epoch 1,Step [6046/12942],950.7717287540436s, Loss:1.9066\n",
      "Epoch 1,Step [6047/12942],986.0311803817749s, Loss:2.3643\n",
      "Epoch 1,Step [6048/12942],1024.3651731014252s, Loss:1.8367\n",
      "Epoch 1,Step [6049/12942],1056.447257757187s, Loss:2.3976\n",
      "Epoch 1,Step [6050/12942],1090.05104970932s, Loss:2.6016\n",
      "Epoch 1,Step [6051/12942],1119.5255138874054s, Loss:2.6084\n",
      "Epoch 1,Step [6052/12942],1149.0488948822021s, Loss:2.8330\n",
      "Epoch 1,Step [6053/12942],1172.6511087417603s, Loss:3.1778\n",
      "Epoch 1,Step [6054/12942],1177.8787212371826s, Loss:2.2300\n",
      "Epoch 1,Step [6055/12942],1182.8872787952423s, Loss:1.8370\n",
      "Epoch 1,Step [6056/12942],1188.142778635025s, Loss:2.1288\n",
      "Epoch 1,Step [6057/12942],1193.339262008667s, Loss:2.0745\n",
      "Epoch 1,Step [6058/12942],1198.5401401519775s, Loss:2.0206\n",
      "Epoch 1,Step [6059/12942],1207.3796977996826s, Loss:2.3311\n",
      "Epoch 1,Step [6060/12942],1237.3502523899078s, Loss:2.0098\n",
      "Epoch 1,Step [6061/12942],1268.6911783218384s, Loss:2.1219\n",
      "Epoch 1,Step [6062/12942],1299.5938951969147s, Loss:2.0110\n",
      "Epoch 1,Step [6063/12942],1329.9863834381104s, Loss:2.1212\n",
      "Epoch 1,Step [6064/12942],1371.1833143234253s, Loss:2.0532\n",
      "Epoch 1,Step [6065/12942],1403.953935623169s, Loss:1.9924\n",
      "Epoch 1,Step [6066/12942],1437.8282558918s, Loss:2.2526\n",
      "Epoch 1,Step [6067/12942],1468.631026506424s, Loss:1.9496\n",
      "Epoch 1,Step [6068/12942],1473.9109144210815s, Loss:2.2418\n",
      "Epoch 1,Step [6069/12942],1479.179220199585s, Loss:2.3845\n",
      "Epoch 1,Step [6070/12942],1484.4315195083618s, Loss:1.8518\n",
      "Epoch 1,Step [6071/12942],1489.6216518878937s, Loss:2.2925\n",
      "Epoch 1,Step [6072/12942],1494.808359861374s, Loss:2.3579\n",
      "Epoch 1,Step [6073/12942],1500.6515564918518s, Loss:2.4401\n",
      "Epoch 1,Step [6074/12942],1513.6561832427979s, Loss:2.6921\n",
      "Epoch 1,Step [6075/12942],1553.648360967636s, Loss:1.8993\n",
      "Epoch 1,Step [6076/12942],1585.7434403896332s, Loss:2.2548\n",
      "Epoch 1,Step [6077/12942],1615.1603786945343s, Loss:2.2518\n",
      "Epoch 1,Step [6078/12942],1647.6091303825378s, Loss:2.0728\n",
      "Epoch 1,Step [6079/12942],1680.8610577583313s, Loss:1.9759\n",
      "Epoch 1,Step [6080/12942],1721.5934972763062s, Loss:2.3725\n",
      "Epoch 1,Step [6081/12942],1751.9345064163208s, Loss:2.3760\n",
      "Epoch 1,Step [6082/12942],1770.7540018558502s, Loss:2.1561\n",
      "Epoch 1,Step [6083/12942],1775.9273781776428s, Loss:2.0757\n",
      "Epoch 1,Step [6084/12942],1781.1691496372223s, Loss:2.3585\n",
      "Epoch 1,Step [6085/12942],1785.9979467391968s, Loss:2.0354\n",
      "Epoch 1,Step [6086/12942],1791.199701309204s, Loss:2.1664\n",
      "Epoch 1,Step [6087/12942],1796.2923457622528s, Loss:2.3477\n",
      "Epoch 1,Step [6088/12942],1802.824046611786s, Loss:1.9185\n",
      "Epoch 1,Step [6089/12942],1824.33181142807s, Loss:2.1102\n",
      "Epoch 1,Step [6090/12942],1856.7378044128418s, Loss:1.9457\n",
      "Epoch 1,Step [6091/12942],1896.7679212093353s, Loss:1.9985\n",
      "Epoch 1,Step [6092/12942],1929.5280199050903s, Loss:2.2562\n",
      "Epoch 1,Step [6093/12942],1959.8125576972961s, Loss:2.1328\n",
      "Epoch 1,Step [6094/12942],1993.0153651237488s, Loss:2.6245\n",
      "Epoch 1,Step [6095/12942],2024.0603845119476s, Loss:1.8159\n",
      "Epoch 1,Step [6096/12942],2054.358669757843s, Loss:2.0586\n",
      "Epoch 1,Step [6097/12942],2072.956735610962s, Loss:2.4407\n",
      "Epoch 1,Step [6098/12942],2078.2918350696564s, Loss:2.8339\n",
      "Epoch 1,Step [6099/12942],2083.470426797867s, Loss:2.4441\n",
      "Epoch 1,Step [6100/12942],2088.717581510544s, Loss:2.4335\n",
      "Epoch 1,Step [6101/12942],2093.548586845398s, Loss:2.1036\n",
      "Epoch 1,Step [6102/12942],2098.6730580329895s, Loss:1.9204\n",
      "Epoch 1,Step [6103/12942],2107.780965566635s, Loss:2.4009\n",
      "Epoch 1,Step [6104/12942],2140.955402135849s, Loss:2.1741\n",
      "Epoch 1,Step [6105/12942],2172.9867889881134s, Loss:2.0199\n",
      "Epoch 1,Step [6106/12942],2204.23646736145s, Loss:2.3244\n",
      "Epoch 1,Step [6107/12942],2237.7179350852966s, Loss:2.5297\n",
      "Epoch 1,Step [6108/12942],2277.285697221756s, Loss:1.9494\n",
      "Epoch 1,Step [6109/12942],2309.3402166366577s, Loss:2.0484\n",
      "Epoch 1,Step [6110/12942],2341.4528801441193s, Loss:2.1033\n",
      "Epoch 1,Step [6111/12942],2369.45401597023s, Loss:2.0317\n",
      "Epoch 1,Step [6112/12942],2374.6720020771027s, Loss:2.1378\n",
      "Epoch 1,Step [6113/12942],2379.9149765968323s, Loss:2.1362\n",
      "Epoch 1,Step [6114/12942],2385.1278858184814s, Loss:2.1951\n",
      "Epoch 1,Step [6115/12942],2390.3866357803345s, Loss:2.2875\n",
      "Epoch 1,Step [6116/12942],2395.6407725811005s, Loss:2.3042\n",
      "Epoch 1,Step [6117/12942],2401.2403650283813s, Loss:2.0389\n",
      "Epoch 1,Step [6118/12942],2415.6080062389374s, Loss:2.3446\n",
      "Epoch 1,Step [6119/12942],2454.7327225208282s, Loss:2.1003\n",
      "Epoch 1,Step [6120/12942],2485.8523030281067s, Loss:2.6919\n",
      "Epoch 1,Step [6121/12942],2517.4298915863037s, Loss:1.9860\n",
      "Epoch 1,Step [6122/12942],2548.670008420944s, Loss:2.5810\n",
      "Epoch 1,Step [6123/12942],2579.553189754486s, Loss:2.2522\n",
      "Epoch 1,Step [6124/12942],2617.22970867157s, Loss:2.3202\n",
      "Epoch 1,Step [6125/12942],2651.8948678970337s, Loss:2.3326\n",
      "Epoch 1,Step [6126/12942],2671.0911910533905s, Loss:2.0912\n",
      "Epoch 1,Step [6127/12942],2676.154796600342s, Loss:2.5356\n",
      "Epoch 1,Step [6128/12942],2681.4106755256653s, Loss:2.1069\n",
      "Epoch 1,Step [6129/12942],2686.6499757766724s, Loss:2.3374\n",
      "Epoch 1,Step [6130/12942],2691.860378742218s, Loss:2.2635\n",
      "Epoch 1,Step [6131/12942],2697.127285003662s, Loss:2.2394\n",
      "Epoch 1,Step [6132/12942],2704.3055391311646s, Loss:2.3804\n",
      "Epoch 1,Step [6133/12942],2729.388955593109s, Loss:2.4315\n",
      "Epoch 1,Step [6134/12942],2761.6962525844574s, Loss:2.6550\n",
      "Epoch 1,Step [6135/12942],2804.607637166977s, Loss:2.2736\n",
      "Epoch 1,Step [6136/12942],2837.12721657753s, Loss:2.9438\n",
      "Epoch 1,Step [6137/12942],2869.4751460552216s, Loss:2.3107\n",
      "Epoch 1,Step [6138/12942],2903.391099214554s, Loss:2.1706\n",
      "Epoch 1,Step [6139/12942],2934.6894705295563s, Loss:2.2415\n",
      "Epoch 1,Step [6140/12942],2968.982164144516s, Loss:2.1869\n",
      "Epoch 1,Step [6141/12942],2975.1956310272217s, Loss:2.3042\n",
      "Epoch 1,Step [6142/12942],2980.112446784973s, Loss:2.3125\n",
      "Epoch 1,Step [6143/12942],2985.2478969097137s, Loss:2.2691\n",
      "Epoch 1,Step [6144/12942],2990.298213005066s, Loss:2.4884\n",
      "Epoch 1,Step [6145/12942],2995.391319513321s, Loss:2.0400\n",
      "Epoch 1,Step [6146/12942],3001.5031757354736s, Loss:2.1607\n",
      "Epoch 1,Step [6147/12942],3018.95884680748s, Loss:2.3226\n",
      "Epoch 1,Step [6148/12942],3051.1284635066986s, Loss:2.3020\n",
      "Epoch 1,Step [6149/12942],3082.777599334717s, Loss:2.2264\n",
      "Epoch 1,Step [6150/12942],3114.458112478256s, Loss:2.0664\n",
      "Epoch 1,Step [6151/12942],3150.1696214675903s, Loss:2.1103\n",
      "Epoch 1,Step [6152/12942],3186.376667022705s, Loss:2.1616\n",
      "Epoch 1,Step [6153/12942],3217.363851070404s, Loss:2.3051\n",
      "Epoch 1,Step [6154/12942],3249.4943277835846s, Loss:2.3342\n",
      "Epoch 1,Step [6155/12942],3270.515783071518s, Loss:2.0403\n",
      "Epoch 1,Step [6156/12942],3275.6875183582306s, Loss:2.2106\n",
      "Epoch 1,Step [6157/12942],3280.923363685608s, Loss:2.6997\n",
      "Epoch 1,Step [6158/12942],3285.9549136161804s, Loss:2.0784\n",
      "Epoch 1,Step [6159/12942],3290.9588787555695s, Loss:2.0517\n",
      "Epoch 1,Step [6160/12942],3296.1803522109985s, Loss:2.2538\n",
      "Epoch 1,Step [6161/12942],3302.6533563137054s, Loss:2.0381\n",
      "Epoch 1,Step [6162/12942],3326.774083852768s, Loss:2.3943\n",
      "Epoch 1,Step [6163/12942],3364.711037874222s, Loss:2.3393\n",
      "Epoch 1,Step [6164/12942],3396.8259143829346s, Loss:2.0546\n",
      "Epoch 1,Step [6165/12942],3428.7653119564056s, Loss:2.5265\n",
      "Epoch 1,Step [6166/12942],3460.775271654129s, Loss:2.3554\n",
      "Epoch 1,Step [6167/12942],3491.7157628536224s, Loss:2.1838\n",
      "Epoch 1,Step [6168/12942],3531.5054938793182s, Loss:2.1480\n",
      "Epoch 1,Step [6169/12942],3563.4650132656097s, Loss:3.0715\n",
      "Epoch 1,Step [6170/12942],3573.122896671295s, Loss:1.8519\n",
      "Epoch 1,Step [6171/12942],3578.3809463977814s, Loss:2.0799\n",
      "Epoch 1,Step [6172/12942],3583.646472454071s, Loss:2.2462\n",
      "Epoch 1,Step [6173/12942],3588.8078758716583s, Loss:2.1783\n",
      "Epoch 1,Step [6174/12942],3594.0835032463074s, Loss:2.5403\n",
      "Epoch 1,Step [6175/12942],3599.6038885116577s, Loss:2.3063\n",
      "Epoch 1,Step [6176/12942],3611.0703432559967s, Loss:2.9645\n",
      "Epoch 1,Step [6177/12942],3642.1764674186707s, Loss:2.2917\n",
      "Epoch 1,Step [6178/12942],3672.530235528946s, Loss:1.9610\n",
      "Epoch 1,Step [6179/12942],3711.8480253219604s, Loss:2.2065\n",
      "Epoch 1,Step [6180/12942],3744.4935686588287s, Loss:2.4931\n",
      "Epoch 1,Step [6181/12942],3776.5999612808228s, Loss:1.8679\n",
      "Epoch 1,Step [6182/12942],3808.875861644745s, Loss:2.5415\n",
      "Epoch 1,Step [6183/12942],3841.1925897598267s, Loss:2.3541\n",
      "Epoch 1,Step [6184/12942],3870.6624460220337s, Loss:2.2316\n",
      "Epoch 1,Step [6185/12942],3875.823812484741s, Loss:2.0840\n",
      "Epoch 1,Step [6186/12942],3881.1430275440216s, Loss:2.9306\n",
      "Epoch 1,Step [6187/12942],3886.039111852646s, Loss:2.1873\n",
      "Epoch 1,Step [6188/12942],3891.0021438598633s, Loss:2.3918\n",
      "Epoch 1,Step [6189/12942],3895.88951420784s, Loss:2.1715\n",
      "Epoch 1,Step [6190/12942],3901.834865808487s, Loss:2.2238\n",
      "Epoch 1,Step [6191/12942],3920.1793999671936s, Loss:2.1606\n",
      "Epoch 1,Step [6192/12942],3953.7352855205536s, Loss:2.0163\n",
      "Epoch 1,Step [6193/12942],3985.6919314861298s, Loss:2.1516\n",
      "Epoch 1,Step [6194/12942],4019.4125323295593s, Loss:2.0467\n",
      "Epoch 1,Step [6195/12942],4060.2572333812714s, Loss:2.1588\n",
      "Epoch 1,Step [6196/12942],4091.1599786281586s, Loss:2.1282\n",
      "Epoch 1,Step [6197/12942],4122.83505153656s, Loss:2.2234\n",
      "Epoch 1,Step [6198/12942],4154.345091819763s, Loss:1.9076\n",
      "Epoch 1,Step [6199/12942],4171.611008882523s, Loss:2.3737\n",
      "Epoch 1,Step [6200/12942],4176.570222854614s, Loss:2.1771\n",
      "Epoch 1,Step [6201/12942],4181.606791496277s, Loss:2.1588\n",
      "Epoch 1,Step [6202/12942],4186.868013858795s, Loss:2.1383\n",
      "Epoch 1,Step [6203/12942],4192.076920747757s, Loss:2.3470\n",
      "Epoch 1,Step [6204/12942],4197.345254898071s, Loss:2.5260\n",
      "Epoch 1,Step [6205/12942],4203.625019311905s, Loss:2.1971\n",
      "Epoch 1,Step [6206/12942],4230.337751150131s, Loss:2.2136\n",
      "Epoch 1,Step [6207/12942],4266.546003818512s, Loss:1.8850\n",
      "Epoch 1,Step [6208/12942],4297.385530948639s, Loss:2.3101\n",
      "Epoch 1,Step [6209/12942],4329.361088514328s, Loss:2.1616\n",
      "Epoch 1,Step [6210/12942],4362.391148805618s, Loss:2.4137\n",
      "Epoch 1,Step [6211/12942],4394.268065690994s, Loss:2.9665\n",
      "Epoch 1,Step [6212/12942],4435.04542350769s, Loss:2.2365\n",
      "Epoch 1,Step [6213/12942],4466.672004938126s, Loss:2.4148\n",
      "Epoch 1,Step [6214/12942],4473.401226043701s, Loss:2.4460\n",
      "Epoch 1,Step [6215/12942],4478.703812122345s, Loss:2.4774\n",
      "Epoch 1,Step [6216/12942],4483.846870183945s, Loss:2.1423\n",
      "Epoch 1,Step [6217/12942],4489.026943445206s, Loss:2.3486\n",
      "Epoch 1,Step [6218/12942],4494.31263422966s, Loss:2.0930\n",
      "Epoch 1,Step [6219/12942],4499.946538925171s, Loss:2.0837\n",
      "Epoch 1,Step [6220/12942],4509.917660474777s, Loss:2.4109\n",
      "Epoch 1,Step [6221/12942],4542.976245641708s, Loss:1.8548\n",
      "Epoch 1,Step [6222/12942],4574.839835882187s, Loss:2.4819\n",
      "Epoch 1,Step [6223/12942],4615.390729665756s, Loss:2.2208\n",
      "Epoch 1,Step [6224/12942],4647.586684703827s, Loss:2.3427\n",
      "Epoch 1,Step [6225/12942],4680.170286178589s, Loss:2.3811\n",
      "Epoch 1,Step [6226/12942],4710.888370275497s, Loss:2.0526\n",
      "Epoch 1,Step [6227/12942],4739.96654176712s, Loss:1.8210\n",
      "Epoch 1,Step [6228/12942],4770.0666716098785s, Loss:2.1365\n",
      "Epoch 1,Step [6229/12942],4775.460057735443s, Loss:2.1276\n",
      "Epoch 1,Step [6230/12942],4780.6667466163635s, Loss:2.1610\n",
      "Epoch 1,Step [6231/12942],4785.857318162918s, Loss:2.2674\n",
      "Epoch 1,Step [6232/12942],4791.16378736496s, Loss:1.8865\n",
      "Epoch 1,Step [6233/12942],4796.404115438461s, Loss:2.2357\n",
      "Epoch 1,Step [6234/12942],4802.793789386749s, Loss:1.9150\n",
      "Epoch 1,Step [6235/12942],4824.426656484604s, Loss:2.0998\n",
      "Epoch 1,Step [6236/12942],4857.936287879944s, Loss:2.6786\n",
      "Epoch 1,Step [6237/12942],4890.831575393677s, Loss:2.2437\n",
      "Epoch 1,Step [6238/12942],4924.522028684616s, Loss:2.6045\n",
      "Epoch 1,Step [6239/12942],4966.192372083664s, Loss:2.6453\n",
      "Epoch 1,Step [6240/12942],4997.937857866287s, Loss:2.7735\n",
      "Epoch 1,Step [6241/12942],5029.326968431473s, Loss:2.0517\n",
      "Epoch 1,Step [6242/12942],5061.227940559387s, Loss:2.7579\n",
      "Epoch 1,Step [6243/12942],5072.7724730968475s, Loss:2.7685\n",
      "Epoch 1,Step [6244/12942],5077.986622810364s, Loss:2.2718\n",
      "Epoch 1,Step [6245/12942],5083.198322057724s, Loss:2.2961\n",
      "Epoch 1,Step [6246/12942],5088.447275876999s, Loss:2.3988\n",
      "Epoch 1,Step [6247/12942],5093.706385612488s, Loss:2.1829\n",
      "Epoch 1,Step [6248/12942],5098.731480121613s, Loss:2.3075\n",
      "Epoch 1,Step [6249/12942],5108.074730634689s, Loss:2.0020\n",
      "Epoch 1,Step [6250/12942],5149.640819787979s, Loss:2.2868\n",
      "Epoch 1,Step [6251/12942],5180.594249486923s, Loss:1.9644\n",
      "Epoch 1,Step [6252/12942],5213.933514356613s, Loss:1.9824\n",
      "Epoch 1,Step [6253/12942],5245.769191741943s, Loss:2.4598\n",
      "Epoch 1,Step [6254/12942],5278.806125402451s, Loss:2.1047\n",
      "Epoch 1,Step [6255/12942],5318.044123649597s, Loss:1.9348\n",
      "Epoch 1,Step [6256/12942],5351.542090415955s, Loss:2.1188\n",
      "Epoch 1,Step [6257/12942],5371.149185180664s, Loss:2.5942\n",
      "Epoch 1,Step [6258/12942],5376.2211101055145s, Loss:2.4126\n",
      "Epoch 1,Step [6259/12942],5381.490884304047s, Loss:2.0911\n",
      "Epoch 1,Step [6260/12942],5386.72985458374s, Loss:2.5207\n",
      "Epoch 1,Step [6261/12942],5391.955317020416s, Loss:2.3302\n",
      "Epoch 1,Step [6262/12942],5397.158433437347s, Loss:2.2420\n",
      "Epoch 1,Step [6263/12942],5404.273201227188s, Loss:2.7838\n",
      "Epoch 1,Step [6264/12942],5427.25576877594s, Loss:2.6636\n",
      "Epoch 1,Step [6265/12942],5458.233639955521s, Loss:2.1080\n",
      "Epoch 1,Step [6266/12942],5498.006321430206s, Loss:2.6601\n",
      "Epoch 1,Step [6267/12942],5531.352570772171s, Loss:1.9431\n",
      "Epoch 1,Step [6268/12942],5564.189553022385s, Loss:2.3146\n",
      "Epoch 1,Step [6269/12942],5596.1901643276215s, Loss:1.9842\n",
      "Epoch 1,Step [6270/12942],5628.685785770416s, Loss:2.2765\n",
      "Epoch 1,Step [6271/12942],5662.657929182053s, Loss:1.9967\n",
      "Epoch 1,Step [6272/12942],5674.294832706451s, Loss:2.4255\n",
      "Epoch 1,Step [6273/12942],5679.314725637436s, Loss:2.2499\n",
      "Epoch 1,Step [6274/12942],5684.520395278931s, Loss:2.1370\n",
      "Epoch 1,Step [6275/12942],5689.8895852565765s, Loss:2.8088\n",
      "Epoch 1,Step [6276/12942],5694.955184698105s, Loss:3.1223\n",
      "Epoch 1,Step [6277/12942],5700.638802289963s, Loss:2.4791\n",
      "Epoch 1,Step [6278/12942],5716.6816375255585s, Loss:2.0186\n",
      "Epoch 1,Step [6279/12942],5747.97132563591s, Loss:2.7245\n",
      "Epoch 1,Step [6280/12942],5779.605554103851s, Loss:2.3675\n",
      "Epoch 1,Step [6281/12942],5810.5663113594055s, Loss:2.0342\n",
      "Epoch 1,Step [6282/12942],5847.48702788353s, Loss:2.3254\n",
      "Epoch 1,Step [6283/12942],5882.425771713257s, Loss:2.1550\n",
      "Epoch 1,Step [6284/12942],5916.650361537933s, Loss:2.6456\n",
      "Epoch 1,Step [6285/12942],5947.679869890213s, Loss:1.9568\n",
      "Epoch 1,Step [6286/12942],5970.085628032684s, Loss:2.0108\n",
      "Epoch 1,Step [6287/12942],5975.260608434677s, Loss:2.0842\n",
      "Epoch 1,Step [6288/12942],5980.44723534584s, Loss:2.2157\n",
      "Epoch 1,Step [6289/12942],5985.660076856613s, Loss:2.0889\n",
      "Epoch 1,Step [6290/12942],5990.8534235954285s, Loss:2.3793\n",
      "Epoch 1,Step [6291/12942],5996.116089582443s, Loss:2.0515\n",
      "Epoch 1,Step [6292/12942],6002.605043888092s, Loss:2.1371\n",
      "Epoch 1,Step [6293/12942],6028.626749515533s, Loss:2.8390\n",
      "Epoch 1,Step [6294/12942],6064.682629108429s, Loss:2.7158\n",
      "Epoch 1,Step [6295/12942],6096.439293861389s, Loss:2.0618\n",
      "Epoch 1,Step [6296/12942],6128.513541221619s, Loss:2.0958\n",
      "Epoch 1,Step [6297/12942],6159.809417963028s, Loss:2.3054\n",
      "Epoch 1,Step [6298/12942],6193.409676790237s, Loss:3.5763\n",
      "Epoch 1,Step [6299/12942],6234.6409866809845s, Loss:2.2605\n",
      "Epoch 1,Step [6300/12942],6266.266942024231s, Loss:2.2287\n",
      "Epoch 1,Step [6300/12942],6266.267985582352s, Loss:2.2287\n",
      "\n",
      "Epoch 1,Step [6301/12942],6.748136758804321s, Loss:2.2549\n",
      "Epoch 1,Step [6302/12942],11.99899172782898s, Loss:2.0019\n",
      "Epoch 1,Step [6303/12942],17.405465602874756s, Loss:2.7754\n",
      "Epoch 1,Step [6304/12942],22.61263918876648s, Loss:2.2109\n",
      "Epoch 1,Step [6305/12942],27.86117386817932s, Loss:2.0358\n",
      "Epoch 1,Step [6306/12942],33.68291759490967s, Loss:2.1326\n",
      "Epoch 1,Step [6307/12942],46.14245367050171s, Loss:2.2147\n",
      "Epoch 1,Step [6308/12942],76.75086712837219s, Loss:2.6737\n",
      "Epoch 1,Step [6309/12942],108.48685717582703s, Loss:2.0736\n",
      "Epoch 1,Step [6310/12942],146.37033343315125s, Loss:2.2214\n",
      "Epoch 1,Step [6311/12942],177.40281414985657s, Loss:1.8541\n",
      "Epoch 1,Step [6312/12942],208.43047952651978s, Loss:1.9892\n",
      "Epoch 1,Step [6313/12942],240.45905804634094s, Loss:2.4939\n",
      "Epoch 1,Step [6314/12942],274.1107313632965s, Loss:2.5147\n",
      "Epoch 1,Step [6315/12942],303.5366356372833s, Loss:1.9730\n",
      "Epoch 1,Step [6316/12942],309.2027771472931s, Loss:2.5905\n",
      "Epoch 1,Step [6317/12942],314.4340600967407s, Loss:1.9019\n",
      "Epoch 1,Step [6318/12942],319.2606580257416s, Loss:2.5647\n",
      "Epoch 1,Step [6319/12942],324.4544563293457s, Loss:2.1769\n",
      "Epoch 1,Step [6320/12942],329.68851041793823s, Loss:2.1049\n",
      "Epoch 1,Step [6321/12942],336.0012905597687s, Loss:2.1982\n",
      "Epoch 1,Step [6322/12942],356.35922932624817s, Loss:2.4423\n",
      "Epoch 1,Step [6323/12942],386.6581561565399s, Loss:2.5614\n",
      "Epoch 1,Step [6324/12942],416.3978707790375s, Loss:2.3401\n",
      "Epoch 1,Step [6325/12942],447.45838809013367s, Loss:2.4611\n",
      "Epoch 1,Step [6326/12942],486.5175721645355s, Loss:2.4357\n",
      "Epoch 1,Step [6327/12942],520.4650702476501s, Loss:2.3032\n",
      "Epoch 1,Step [6328/12942],551.6798024177551s, Loss:1.8982\n",
      "Epoch 1,Step [6329/12942],583.7773251533508s, Loss:2.6128\n",
      "Epoch 1,Step [6330/12942],604.1114835739136s, Loss:2.1298\n",
      "Epoch 1,Step [6331/12942],609.3369243144989s, Loss:2.0581\n",
      "Epoch 1,Step [6332/12942],614.6026933193207s, Loss:1.6478\n",
      "Epoch 1,Step [6333/12942],619.8246791362762s, Loss:2.5021\n",
      "Epoch 1,Step [6334/12942],625.1069395542145s, Loss:2.2228\n",
      "Epoch 1,Step [6335/12942],630.3410787582397s, Loss:2.1568\n",
      "Epoch 1,Step [6336/12942],637.1909303665161s, Loss:2.1308\n",
      "Epoch 1,Step [6337/12942],668.3722820281982s, Loss:2.3792\n",
      "Epoch 1,Step [6338/12942],702.7648432254791s, Loss:2.2997\n",
      "Epoch 1,Step [6339/12942],734.8602361679077s, Loss:2.3032\n",
      "Epoch 1,Step [6340/12942],766.1525321006775s, Loss:2.9652\n",
      "Epoch 1,Step [6341/12942],798.0006256103516s, Loss:2.0983\n",
      "Epoch 1,Step [6342/12942],832.2082901000977s, Loss:2.1036\n",
      "Epoch 1,Step [6343/12942],871.2661266326904s, Loss:2.3430\n",
      "Epoch 1,Step [6344/12942],902.0762431621552s, Loss:2.4735\n",
      "Epoch 1,Step [6345/12942],907.2959797382355s, Loss:2.1298\n",
      "Epoch 1,Step [6346/12942],912.6138620376587s, Loss:2.3339\n",
      "Epoch 1,Step [6347/12942],917.4269802570343s, Loss:2.0917\n",
      "Epoch 1,Step [6348/12942],922.6252689361572s, Loss:2.0926\n",
      "Epoch 1,Step [6349/12942],927.8904995918274s, Loss:2.2478\n",
      "Epoch 1,Step [6350/12942],933.6211524009705s, Loss:2.3044\n",
      "Epoch 1,Step [6351/12942],947.5413677692413s, Loss:2.8593\n",
      "Epoch 1,Step [6352/12942],978.0957691669464s, Loss:2.2235\n",
      "Epoch 1,Step [6353/12942],1007.7862160205841s, Loss:1.9896\n",
      "Epoch 1,Step [6354/12942],1048.5449934005737s, Loss:2.4423\n",
      "Epoch 1,Step [6355/12942],1082.0614507198334s, Loss:2.2749\n",
      "Epoch 1,Step [6356/12942],1112.0861191749573s, Loss:2.0520\n",
      "Epoch 1,Step [6357/12942],1146.7080245018005s, Loss:2.2196\n",
      "Epoch 1,Step [6358/12942],1178.7710728645325s, Loss:2.3812\n",
      "Epoch 1,Step [6359/12942],1204.4897894859314s, Loss:2.1109\n",
      "Epoch 1,Step [6360/12942],1209.5189514160156s, Loss:2.0092\n",
      "Epoch 1,Step [6361/12942],1214.759292125702s, Loss:2.0877\n",
      "Epoch 1,Step [6362/12942],1219.9890031814575s, Loss:2.0040\n",
      "Epoch 1,Step [6363/12942],1225.1209115982056s, Loss:2.2029\n",
      "Epoch 1,Step [6364/12942],1230.3497879505157s, Loss:2.3364\n",
      "Epoch 1,Step [6365/12942],1237.894880771637s, Loss:2.0943\n",
      "Epoch 1,Step [6366/12942],1261.5145692825317s, Loss:2.1971\n",
      "Epoch 1,Step [6367/12942],1293.6033239364624s, Loss:2.1193\n",
      "Epoch 1,Step [6368/12942],1325.5623819828033s, Loss:2.0942\n",
      "Epoch 1,Step [6369/12942],1359.5560371875763s, Loss:2.4885\n",
      "Epoch 1,Step [6370/12942],1398.4933125972748s, Loss:2.5183\n",
      "Epoch 1,Step [6371/12942],1429.025095462799s, Loss:2.4471\n",
      "Epoch 1,Step [6372/12942],1462.3744192123413s, Loss:2.1576\n",
      "Epoch 1,Step [6373/12942],1495.2276165485382s, Loss:2.1751\n",
      "Epoch 1,Step [6374/12942],1506.1601068973541s, Loss:2.2766\n",
      "Epoch 1,Step [6375/12942],1511.347584247589s, Loss:2.2354\n",
      "Epoch 1,Step [6376/12942],1516.4067876338959s, Loss:2.2658\n",
      "Epoch 1,Step [6377/12942],1521.5298125743866s, Loss:1.8963\n",
      "Epoch 1,Step [6378/12942],1526.8794326782227s, Loss:2.8347\n",
      "Epoch 1,Step [6379/12942],1532.1689581871033s, Loss:2.0133\n",
      "Epoch 1,Step [6380/12942],1541.1854181289673s, Loss:2.3479\n",
      "Epoch 1,Step [6381/12942],1580.671126127243s, Loss:2.1903\n",
      "Epoch 1,Step [6382/12942],1611.6316533088684s, Loss:2.0693\n",
      "Epoch 1,Step [6383/12942],1642.9962916374207s, Loss:1.9468\n",
      "Epoch 1,Step [6384/12942],1675.370700120926s, Loss:2.3451\n",
      "Epoch 1,Step [6385/12942],1707.6491572856903s, Loss:2.5238\n",
      "Epoch 1,Step [6386/12942],1742.3004097938538s, Loss:2.2473\n",
      "Epoch 1,Step [6387/12942],1775.188283443451s, Loss:2.4412\n",
      "Epoch 1,Step [6388/12942],1802.820413351059s, Loss:2.1487\n",
      "Epoch 1,Step [6389/12942],1807.9270994663239s, Loss:1.9112\n",
      "Epoch 1,Step [6390/12942],1813.1901824474335s, Loss:2.3811\n",
      "Epoch 1,Step [6391/12942],1818.1998188495636s, Loss:2.0075\n",
      "Epoch 1,Step [6392/12942],1823.2970991134644s, Loss:2.1567\n",
      "Epoch 1,Step [6393/12942],1828.5492279529572s, Loss:2.0182\n",
      "Epoch 1,Step [6394/12942],1834.4562327861786s, Loss:2.4873\n",
      "Epoch 1,Step [6395/12942],1850.4999384880066s, Loss:2.2884\n",
      "Epoch 1,Step [6396/12942],1884.4266872406006s, Loss:2.0887\n",
      "Epoch 1,Step [6397/12942],1920.2816882133484s, Loss:2.5388\n",
      "Epoch 1,Step [6398/12942],1955.307270526886s, Loss:1.9587\n",
      "Epoch 1,Step [6399/12942],1987.2603414058685s, Loss:2.2274\n",
      "Epoch 1,Step [6400/12942],2019.4883134365082s, Loss:2.3592\n",
      "Epoch 1,Step [6401/12942],2052.639145374298s, Loss:2.0343\n",
      "Epoch 1,Step [6402/12942],2084.7578065395355s, Loss:2.3174\n",
      "Epoch 1,Step [6403/12942],2106.010200023651s, Loss:2.4717\n",
      "Epoch 1,Step [6404/12942],2111.0562155246735s, Loss:2.2908\n",
      "Epoch 1,Step [6405/12942],2116.3044521808624s, Loss:2.3171\n",
      "Epoch 1,Step [6406/12942],2121.52632689476s, Loss:2.3520\n",
      "Epoch 1,Step [6407/12942],2126.8422389030457s, Loss:2.1794\n",
      "Epoch 1,Step [6408/12942],2132.0786452293396s, Loss:2.5833\n",
      "Epoch 1,Step [6409/12942],2141.7999289035797s, Loss:2.0037\n",
      "Epoch 1,Step [6410/12942],2174.501561164856s, Loss:2.3507\n",
      "Epoch 1,Step [6411/12942],2204.726475715637s, Loss:2.0518\n",
      "Epoch 1,Step [6412/12942],2236.715253353119s, Loss:2.2087\n",
      "Epoch 1,Step [6413/12942],2269.0726096630096s, Loss:3.0515\n",
      "Epoch 1,Step [6414/12942],2307.116943359375s, Loss:2.0228\n",
      "Epoch 1,Step [6415/12942],2336.249634742737s, Loss:2.1501\n",
      "Epoch 1,Step [6416/12942],2367.797174692154s, Loss:2.3319\n",
      "Epoch 1,Step [6417/12942],2399.457428216934s, Loss:2.2854\n",
      "Epoch 1,Step [6418/12942],2406.720356464386s, Loss:2.5382\n",
      "Epoch 1,Step [6419/12942],2411.9944157600403s, Loss:2.3795\n",
      "Epoch 1,Step [6420/12942],2417.2404069900513s, Loss:2.2196\n",
      "Epoch 1,Step [6421/12942],2422.175094127655s, Loss:2.0541\n",
      "Epoch 1,Step [6422/12942],2427.320387363434s, Loss:2.5902\n",
      "Epoch 1,Step [6423/12942],2432.637894153595s, Loss:2.3877\n",
      "Epoch 1,Step [6424/12942],2442.2776403427124s, Loss:2.3002\n",
      "Epoch 1,Step [6425/12942],2483.853526353836s, Loss:2.3315\n",
      "Epoch 1,Step [6426/12942],2516.2255289554596s, Loss:2.0564\n",
      "Epoch 1,Step [6427/12942],2547.9618837833405s, Loss:2.1598\n",
      "Epoch 1,Step [6428/12942],2581.2857434749603s, Loss:2.5012\n",
      "Epoch 1,Step [6429/12942],2613.636267185211s, Loss:1.9576\n",
      "Epoch 1,Step [6430/12942],2654.1302242279053s, Loss:2.3460\n",
      "Epoch 1,Step [6431/12942],2687.3700041770935s, Loss:2.1210\n",
      "Epoch 1,Step [6432/12942],2704.9152567386627s, Loss:2.0348\n",
      "Epoch 1,Step [6433/12942],2710.119261264801s, Loss:2.0720\n",
      "Epoch 1,Step [6434/12942],2715.393371105194s, Loss:2.3308\n",
      "Epoch 1,Step [6435/12942],2720.586111307144s, Loss:2.4398\n",
      "Epoch 1,Step [6436/12942],2725.7233991622925s, Loss:2.0835\n",
      "Epoch 1,Step [6437/12942],2730.935047864914s, Loss:1.9765\n",
      "Epoch 1,Step [6438/12942],2738.734329223633s, Loss:2.0773\n",
      "Epoch 1,Step [6439/12942],2765.8940620422363s, Loss:2.6347\n",
      "Epoch 1,Step [6440/12942],2799.906751394272s, Loss:3.2865\n",
      "Epoch 1,Step [6441/12942],2840.3894593715668s, Loss:2.0394\n",
      "Epoch 1,Step [6442/12942],2871.347977876663s, Loss:2.4739\n",
      "Epoch 1,Step [6443/12942],2903.2125132083893s, Loss:2.1461\n",
      "Epoch 1,Step [6444/12942],2936.325035572052s, Loss:2.0335\n",
      "Epoch 1,Step [6445/12942],2969.020330429077s, Loss:2.2859\n",
      "Epoch 1,Step [6446/12942],3002.7003695964813s, Loss:2.1652\n",
      "Epoch 1,Step [6447/12942],3008.480276107788s, Loss:2.5270\n",
      "Epoch 1,Step [6448/12942],3013.4803063869476s, Loss:2.0320\n",
      "Epoch 1,Step [6449/12942],3018.751499891281s, Loss:2.2626\n",
      "Epoch 1,Step [6450/12942],3023.9466910362244s, Loss:2.0898\n",
      "Epoch 1,Step [6451/12942],3028.968650341034s, Loss:2.4655\n",
      "Epoch 1,Step [6452/12942],3035.083304166794s, Loss:2.2103\n",
      "Epoch 1,Step [6453/12942],3052.054393529892s, Loss:2.2427\n",
      "Epoch 1,Step [6454/12942],3082.356528043747s, Loss:2.3939\n",
      "Epoch 1,Step [6455/12942],3116.780167579651s, Loss:3.0247\n",
      "Epoch 1,Step [6456/12942],3149.1641857624054s, Loss:3.1513\n",
      "Epoch 1,Step [6457/12942],3190.931643009186s, Loss:2.4234\n",
      "Epoch 1,Step [6458/12942],3224.466737508774s, Loss:2.4300\n",
      "Epoch 1,Step [6459/12942],3257.2702689170837s, Loss:2.3235\n",
      "Epoch 1,Step [6460/12942],3289.1741704940796s, Loss:2.5717\n",
      "Epoch 1,Step [6461/12942],3305.193756341934s, Loss:2.2039\n",
      "Epoch 1,Step [6462/12942],3310.442965745926s, Loss:1.9890\n",
      "Epoch 1,Step [6463/12942],3315.4922449588776s, Loss:2.1934\n",
      "Epoch 1,Step [6464/12942],3320.627460718155s, Loss:2.2029\n",
      "Epoch 1,Step [6465/12942],3325.6866567134857s, Loss:2.1447\n",
      "Epoch 1,Step [6466/12942],3330.869972229004s, Loss:2.4841\n",
      "Epoch 1,Step [6467/12942],3338.8309824466705s, Loss:2.1938\n",
      "Epoch 1,Step [6468/12942],3374.591394662857s, Loss:2.5339\n",
      "Epoch 1,Step [6469/12942],3406.695671081543s, Loss:2.2188\n",
      "Epoch 1,Step [6470/12942],3439.4922335147858s, Loss:1.9154\n",
      "Epoch 1,Step [6471/12942],3473.920575618744s, Loss:2.3425\n",
      "Epoch 1,Step [6472/12942],3506.216361284256s, Loss:2.2084\n",
      "Epoch 1,Step [6473/12942],3544.024992465973s, Loss:2.3038\n",
      "Epoch 1,Step [6474/12942],3576.483405351639s, Loss:2.5017\n",
      "Epoch 1,Step [6475/12942],3603.0308170318604s, Loss:2.2663\n",
      "Epoch 1,Step [6476/12942],3608.2606902122498s, Loss:2.2883\n",
      "Epoch 1,Step [6477/12942],3613.55406498909s, Loss:2.2637\n",
      "Epoch 1,Step [6478/12942],3618.7823688983917s, Loss:2.2387\n",
      "Epoch 1,Step [6479/12942],3623.9448001384735s, Loss:2.5679\n",
      "Epoch 1,Step [6480/12942],3629.0934553146362s, Loss:1.9840\n",
      "Epoch 1,Step [6481/12942],3634.636345386505s, Loss:2.5638\n",
      "Epoch 1,Step [6482/12942],3653.1252822875977s, Loss:2.4932\n",
      "Epoch 1,Step [6483/12942],3686.137809276581s, Loss:2.3379\n",
      "Epoch 1,Step [6484/12942],3723.2962050437927s, Loss:2.1150\n",
      "Epoch 1,Step [6485/12942],3758.591947078705s, Loss:2.6515\n",
      "Epoch 1,Step [6486/12942],3790.483563184738s, Loss:2.1234\n",
      "Epoch 1,Step [6487/12942],3821.0323138237s, Loss:2.4777\n",
      "Epoch 1,Step [6488/12942],3852.4657685756683s, Loss:2.2480\n",
      "Epoch 1,Step [6489/12942],3884.6917493343353s, Loss:1.9536\n",
      "Epoch 1,Step [6490/12942],3906.276701927185s, Loss:2.0359\n",
      "Epoch 1,Step [6491/12942],3911.479383945465s, Loss:2.5181\n",
      "Epoch 1,Step [6492/12942],3916.7433202266693s, Loss:2.5527\n",
      "Epoch 1,Step [6493/12942],3921.9868655204773s, Loss:2.2263\n",
      "Epoch 1,Step [6494/12942],3927.220336675644s, Loss:2.0872\n",
      "Epoch 1,Step [6495/12942],3932.719377040863s, Loss:2.1173\n",
      "Epoch 1,Step [6496/12942],3943.143004655838s, Loss:2.0505\n",
      "Epoch 1,Step [6497/12942],3975.559860944748s, Loss:1.9883\n",
      "Epoch 1,Step [6498/12942],4006.0356950759888s, Loss:2.1047\n",
      "Epoch 1,Step [6499/12942],4035.833734035492s, Loss:2.1123\n",
      "Epoch 1,Step [6500/12942],4068.4596922397614s, Loss:2.3415\n",
      "Epoch 1,Step [6501/12942],4109.08064198494s, Loss:2.0422\n",
      "Epoch 1,Step [6502/12942],4142.20215344429s, Loss:2.1002\n",
      "Epoch 1,Step [6503/12942],4175.032560110092s, Loss:2.2243\n",
      "Epoch 1,Step [6504/12942],4202.923699617386s, Loss:2.2638\n",
      "Epoch 1,Step [6505/12942],4208.197710514069s, Loss:2.4812\n",
      "Epoch 1,Step [6506/12942],4213.329286336899s, Loss:2.6987\n",
      "Epoch 1,Step [6507/12942],4218.522001981735s, Loss:2.4604\n",
      "Epoch 1,Step [6508/12942],4223.716866493225s, Loss:2.2081\n",
      "Epoch 1,Step [6509/12942],4229.053853750229s, Loss:2.6552\n",
      "Epoch 1,Step [6510/12942],4235.105075836182s, Loss:2.1460\n",
      "Epoch 1,Step [6511/12942],4255.7142724990845s, Loss:2.0479\n",
      "Epoch 1,Step [6512/12942],4293.945162057877s, Loss:2.0804\n",
      "Epoch 1,Step [6513/12942],4325.577430009842s, Loss:2.1998\n",
      "Epoch 1,Step [6514/12942],4359.429848909378s, Loss:2.0262\n",
      "Epoch 1,Step [6515/12942],4392.3131420612335s, Loss:2.1002\n",
      "Epoch 1,Step [6516/12942],4425.04855966568s, Loss:2.3303\n",
      "Epoch 1,Step [6517/12942],4464.984538316727s, Loss:2.3239\n",
      "Epoch 1,Step [6518/12942],4495.657168865204s, Loss:1.9609\n",
      "Epoch 1,Step [6519/12942],4506.3629195690155s, Loss:2.1030\n",
      "Epoch 1,Step [6520/12942],4511.647032737732s, Loss:2.0790\n",
      "Epoch 1,Step [6521/12942],4516.933124303818s, Loss:1.9344\n",
      "Epoch 1,Step [6522/12942],4522.169717311859s, Loss:2.6008\n",
      "Epoch 1,Step [6523/12942],4527.533435344696s, Loss:2.2422\n",
      "Epoch 1,Step [6524/12942],4533.785822153091s, Loss:4.7106\n",
      "Epoch 1,Step [6525/12942],4545.061975479126s, Loss:2.1670\n",
      "Epoch 1,Step [6526/12942],4578.271388530731s, Loss:2.1802\n",
      "Epoch 1,Step [6527/12942],4610.777494907379s, Loss:3.1559\n",
      "Epoch 1,Step [6528/12942],4650.377905130386s, Loss:1.9293\n",
      "Epoch 1,Step [6529/12942],4682.220958471298s, Loss:1.9616\n",
      "Epoch 1,Step [6530/12942],4715.776788711548s, Loss:2.8266\n",
      "Epoch 1,Step [6531/12942],4749.273126840591s, Loss:2.2665\n",
      "Epoch 1,Step [6532/12942],4782.704461574554s, Loss:2.2529\n",
      "Epoch 1,Step [6533/12942],4806.076255321503s, Loss:2.5885\n",
      "Epoch 1,Step [6534/12942],4811.053824663162s, Loss:2.3967\n",
      "Epoch 1,Step [6535/12942],4816.275461196899s, Loss:2.1948\n",
      "Epoch 1,Step [6536/12942],4821.562599658966s, Loss:2.0476\n",
      "Epoch 1,Step [6537/12942],4826.835302352905s, Loss:1.9958\n",
      "Epoch 1,Step [6538/12942],4832.234093904495s, Loss:2.0710\n",
      "Epoch 1,Step [6539/12942],4840.678780555725s, Loss:2.1906\n",
      "Epoch 1,Step [6540/12942],4867.633975505829s, Loss:2.5583\n",
      "Epoch 1,Step [6541/12942],4897.003062009811s, Loss:2.3939\n",
      "Epoch 1,Step [6542/12942],4929.534815788269s, Loss:2.4609\n",
      "Epoch 1,Step [6543/12942],4961.16881275177s, Loss:2.1362\n",
      "Epoch 1,Step [6544/12942],5004.8276200294495s, Loss:2.1319\n",
      "Epoch 1,Step [6545/12942],5036.293296575546s, Loss:2.0302\n",
      "Epoch 1,Step [6546/12942],5069.519186258316s, Loss:2.4449\n",
      "Epoch 1,Step [6547/12942],5101.568372011185s, Loss:2.0590\n",
      "Epoch 1,Step [6548/12942],5107.17701458931s, Loss:1.8149\n",
      "Epoch 1,Step [6549/12942],5112.307419061661s, Loss:2.1272\n",
      "Epoch 1,Step [6550/12942],5117.497620105743s, Loss:2.2015\n",
      "Epoch 1,Step [6551/12942],5122.768982887268s, Loss:2.0926\n",
      "Epoch 1,Step [6552/12942],5128.027400016785s, Loss:2.5763\n",
      "Epoch 1,Step [6553/12942],5133.3614184856415s, Loss:2.1126\n",
      "Epoch 1,Step [6554/12942],5144.903962850571s, Loss:2.2259\n",
      "Epoch 1,Step [6555/12942],5186.401741504669s, Loss:2.3489\n",
      "Epoch 1,Step [6556/12942],5218.315133571625s, Loss:2.6881\n",
      "Epoch 1,Step [6557/12942],5248.692959308624s, Loss:2.3724\n",
      "Epoch 1,Step [6558/12942],5280.915214538574s, Loss:2.3871\n",
      "Epoch 1,Step [6559/12942],5312.598737716675s, Loss:2.3369\n",
      "Epoch 1,Step [6560/12942],5351.6862580776215s, Loss:2.2774\n",
      "Epoch 1,Step [6561/12942],5386.258123636246s, Loss:2.7840\n",
      "Epoch 1,Step [6562/12942],5404.769335031509s, Loss:2.1657\n",
      "Epoch 1,Step [6563/12942],5410.024714708328s, Loss:2.0002\n",
      "Epoch 1,Step [6564/12942],5415.216831684113s, Loss:2.2234\n",
      "Epoch 1,Step [6565/12942],5420.463145971298s, Loss:1.9901\n",
      "Epoch 1,Step [6566/12942],5425.680549860001s, Loss:2.5074\n",
      "Epoch 1,Step [6567/12942],5430.679694414139s, Loss:2.9666\n",
      "Epoch 1,Step [6568/12942],5437.468752861023s, Loss:2.3429\n",
      "Epoch 1,Step [6569/12942],5461.059799909592s, Loss:2.0735\n",
      "Epoch 1,Step [6570/12942],5494.291625738144s, Loss:2.4099\n",
      "Epoch 1,Step [6571/12942],5536.978832960129s, Loss:1.9781\n",
      "Epoch 1,Step [6572/12942],5567.048476219177s, Loss:2.2977\n",
      "Epoch 1,Step [6573/12942],5600.619733572006s, Loss:2.1050\n",
      "Epoch 1,Step [6574/12942],5631.94387960434s, Loss:2.0644\n",
      "Epoch 1,Step [6575/12942],5662.637533664703s, Loss:2.0356\n",
      "Epoch 1,Step [6576/12942],5698.649067401886s, Loss:1.9476\n",
      "Epoch 1,Step [6577/12942],5707.719450950623s, Loss:2.7614\n",
      "Epoch 1,Step [6578/12942],5712.918586015701s, Loss:2.1236\n",
      "Epoch 1,Step [6579/12942],5718.19008231163s, Loss:2.2554\n",
      "Epoch 1,Step [6580/12942],5723.056257009506s, Loss:2.0328\n",
      "Epoch 1,Step [6581/12942],5728.225518465042s, Loss:2.3859\n",
      "Epoch 1,Step [6582/12942],5733.905884504318s, Loss:2.4577\n",
      "Epoch 1,Step [6583/12942],5747.641402006149s, Loss:2.3999\n",
      "Epoch 1,Step [6584/12942],5779.410351514816s, Loss:2.1359\n",
      "Epoch 1,Step [6585/12942],5811.253443241119s, Loss:2.2224\n",
      "Epoch 1,Step [6586/12942],5842.409373283386s, Loss:2.3543\n",
      "Epoch 1,Step [6587/12942],5875.078843832016s, Loss:2.0945\n",
      "Epoch 1,Step [6588/12942],5913.400418043137s, Loss:2.1855\n",
      "Epoch 1,Step [6589/12942],5946.346923828125s, Loss:2.1948\n",
      "Epoch 1,Step [6590/12942],5981.77992105484s, Loss:2.2062\n",
      "Epoch 1,Step [6591/12942],6003.804202795029s, Loss:2.4243\n",
      "Epoch 1,Step [6592/12942],6008.9774215221405s, Loss:2.0490\n",
      "Epoch 1,Step [6593/12942],6013.944665193558s, Loss:2.2772\n",
      "Epoch 1,Step [6594/12942],6019.184380531311s, Loss:1.9461\n",
      "Epoch 1,Step [6595/12942],6024.324480056763s, Loss:2.1444\n",
      "Epoch 1,Step [6596/12942],6029.538328409195s, Loss:1.8105\n",
      "Epoch 1,Step [6597/12942],6035.439391374588s, Loss:2.2166\n",
      "Epoch 1,Step [6598/12942],6055.986440420151s, Loss:2.2954\n",
      "Epoch 1,Step [6599/12942],6094.276334285736s, Loss:2.4882\n",
      "Epoch 1,Step [6600/12942],6127.869161605835s, Loss:2.1722\n",
      "Epoch 1,Step [6600/12942],6127.870232820511s, Loss:2.1722\n",
      "\n",
      "Epoch 1,Step [6601/12942],29.88028907775879s, Loss:2.1957\n",
      "Epoch 1,Step [6602/12942],59.74464011192322s, Loss:2.0247\n",
      "Epoch 1,Step [6603/12942],92.6076180934906s, Loss:2.1441\n",
      "Epoch 1,Step [6604/12942],133.40366458892822s, Loss:2.0224\n",
      "Epoch 1,Step [6605/12942],167.3048951625824s, Loss:2.0397\n",
      "Epoch 1,Step [6606/12942],177.93116664886475s, Loss:2.1734\n",
      "Epoch 1,Step [6607/12942],183.1816122531891s, Loss:2.4721\n",
      "Epoch 1,Step [6608/12942],188.45001220703125s, Loss:2.0929\n",
      "Epoch 1,Step [6609/12942],193.73412585258484s, Loss:2.1352\n",
      "Epoch 1,Step [6610/12942],199.05403757095337s, Loss:2.9566\n",
      "Epoch 1,Step [6611/12942],204.55477237701416s, Loss:2.2883\n",
      "Epoch 1,Step [6612/12942],213.41857409477234s, Loss:2.1006\n",
      "Epoch 1,Step [6613/12942],245.68734788894653s, Loss:2.2285\n",
      "Epoch 1,Step [6614/12942],279.22176027297974s, Loss:2.1812\n",
      "Epoch 1,Step [6615/12942],318.9021542072296s, Loss:2.2314\n",
      "Epoch 1,Step [6616/12942],351.13627314567566s, Loss:1.9862\n",
      "Epoch 1,Step [6617/12942],380.4011528491974s, Loss:2.1857\n",
      "Epoch 1,Step [6618/12942],413.58604741096497s, Loss:2.1783\n",
      "Epoch 1,Step [6619/12942],446.0852370262146s, Loss:2.5843\n",
      "Epoch 1,Step [6620/12942],475.852823972702s, Loss:2.1251\n",
      "Epoch 1,Step [6621/12942],481.28322410583496s, Loss:2.0439\n",
      "Epoch 1,Step [6622/12942],486.59952664375305s, Loss:2.1181\n",
      "Epoch 1,Step [6623/12942],491.7232882976532s, Loss:2.6477\n",
      "Epoch 1,Step [6624/12942],496.94690895080566s, Loss:2.3223\n",
      "Epoch 1,Step [6625/12942],502.1660153865814s, Loss:2.2877\n",
      "Epoch 1,Step [6626/12942],509.231906414032s, Loss:2.0304\n",
      "Epoch 1,Step [6627/12942],534.1085884571075s, Loss:2.3576\n",
      "Epoch 1,Step [6628/12942],563.7276513576508s, Loss:1.9545\n",
      "Epoch 1,Step [6629/12942],595.6543254852295s, Loss:2.2619\n",
      "Epoch 1,Step [6630/12942],624.9917545318604s, Loss:2.1593\n",
      "Epoch 1,Step [6631/12942],664.1599886417389s, Loss:2.3290\n",
      "Epoch 1,Step [6632/12942],698.604326248169s, Loss:2.9236\n",
      "Epoch 1,Step [6633/12942],731.3331608772278s, Loss:2.3920\n",
      "Epoch 1,Step [6634/12942],763.4448165893555s, Loss:2.7604\n",
      "Epoch 1,Step [6635/12942],777.3705627918243s, Loss:2.4242\n",
      "Epoch 1,Step [6636/12942],782.580206155777s, Loss:2.1160\n",
      "Epoch 1,Step [6637/12942],787.8425073623657s, Loss:2.4999\n",
      "Epoch 1,Step [6638/12942],793.2183673381805s, Loss:2.9737\n",
      "Epoch 1,Step [6639/12942],798.5248982906342s, Loss:2.4322\n",
      "Epoch 1,Step [6640/12942],803.8487377166748s, Loss:2.0065\n",
      "Epoch 1,Step [6641/12942],812.7363245487213s, Loss:2.1665\n",
      "Epoch 1,Step [6642/12942],850.5885963439941s, Loss:2.0273\n",
      "Epoch 1,Step [6643/12942],884.6955871582031s, Loss:2.0333\n",
      "Epoch 1,Step [6644/12942],918.649505853653s, Loss:2.3094\n",
      "Epoch 1,Step [6645/12942],951.2433640956879s, Loss:1.9508\n",
      "Epoch 1,Step [6646/12942],982.3904213905334s, Loss:2.0728\n",
      "Epoch 1,Step [6647/12942],1022.3731892108917s, Loss:2.3557\n",
      "Epoch 1,Step [6648/12942],1055.734786748886s, Loss:2.1115\n",
      "Epoch 1,Step [6649/12942],1076.0671865940094s, Loss:2.0689\n",
      "Epoch 1,Step [6650/12942],1081.3892078399658s, Loss:2.3022\n",
      "Epoch 1,Step [6651/12942],1086.6792118549347s, Loss:2.1259\n",
      "Epoch 1,Step [6652/12942],1091.5392627716064s, Loss:1.9489\n",
      "Epoch 1,Step [6653/12942],1096.599811077118s, Loss:2.3419\n",
      "Epoch 1,Step [6654/12942],1101.7238051891327s, Loss:2.5407\n",
      "Epoch 1,Step [6655/12942],1108.291226387024s, Loss:2.1692\n",
      "Epoch 1,Step [6656/12942],1129.158196926117s, Loss:2.0103\n",
      "Epoch 1,Step [6657/12942],1160.7903289794922s, Loss:2.1660\n",
      "Epoch 1,Step [6658/12942],1200.0322842597961s, Loss:1.8284\n",
      "Epoch 1,Step [6659/12942],1234.4556908607483s, Loss:2.2098\n",
      "Epoch 1,Step [6660/12942],1265.5233454704285s, Loss:1.8998\n",
      "Epoch 1,Step [6661/12942],1297.7192583084106s, Loss:2.6321\n",
      "Epoch 1,Step [6662/12942],1330.0993673801422s, Loss:2.5112\n",
      "Epoch 1,Step [6663/12942],1360.1050550937653s, Loss:2.4618\n",
      "Epoch 1,Step [6664/12942],1378.4405150413513s, Loss:2.5101\n",
      "Epoch 1,Step [6665/12942],1383.6543128490448s, Loss:2.3741\n",
      "Epoch 1,Step [6666/12942],1388.9126179218292s, Loss:2.1803\n",
      "Epoch 1,Step [6667/12942],1394.147488117218s, Loss:2.1411\n",
      "Epoch 1,Step [6668/12942],1399.0775337219238s, Loss:2.5283\n",
      "Epoch 1,Step [6669/12942],1404.309024810791s, Loss:2.5547\n",
      "Epoch 1,Step [6670/12942],1413.67342710495s, Loss:2.3895\n",
      "Epoch 1,Step [6671/12942],1446.5218172073364s, Loss:2.0002\n",
      "Epoch 1,Step [6672/12942],1479.735550403595s, Loss:2.2252\n",
      "Epoch 1,Step [6673/12942],1512.2144491672516s, Loss:2.1489\n",
      "Epoch 1,Step [6674/12942],1547.2724499702454s, Loss:2.3051\n",
      "Epoch 1,Step [6675/12942],1586.397752046585s, Loss:2.1439\n",
      "Epoch 1,Step [6676/12942],1619.7876121997833s, Loss:2.2686\n",
      "Epoch 1,Step [6677/12942],1652.0113010406494s, Loss:2.1019\n",
      "Epoch 1,Step [6678/12942],1675.2003569602966s, Loss:2.3509\n",
      "Epoch 1,Step [6679/12942],1680.2702414989471s, Loss:1.9620\n",
      "Epoch 1,Step [6680/12942],1685.2127211093903s, Loss:2.1735\n",
      "Epoch 1,Step [6681/12942],1690.5147988796234s, Loss:2.4265\n",
      "Epoch 1,Step [6682/12942],1695.7963950634003s, Loss:2.1050\n",
      "Epoch 1,Step [6683/12942],1701.0995285511017s, Loss:2.3772\n",
      "Epoch 1,Step [6684/12942],1707.3358218669891s, Loss:2.3217\n",
      "Epoch 1,Step [6685/12942],1732.090309381485s, Loss:2.2728\n",
      "Epoch 1,Step [6686/12942],1769.9353971481323s, Loss:1.9839\n",
      "Epoch 1,Step [6687/12942],1800.7758395671844s, Loss:2.3469\n",
      "Epoch 1,Step [6688/12942],1830.7103915214539s, Loss:1.9670\n",
      "Epoch 1,Step [6689/12942],1862.319482088089s, Loss:2.1722\n",
      "Epoch 1,Step [6690/12942],1894.9467432498932s, Loss:2.0521\n",
      "Epoch 1,Step [6691/12942],1935.7383346557617s, Loss:2.1166\n",
      "Epoch 1,Step [6692/12942],1968.613692998886s, Loss:2.3337\n",
      "Epoch 1,Step [6693/12942],1978.1786909103394s, Loss:2.4201\n",
      "Epoch 1,Step [6694/12942],1983.4014658927917s, Loss:2.4943\n",
      "Epoch 1,Step [6695/12942],1988.6599533557892s, Loss:1.8508\n",
      "Epoch 1,Step [6696/12942],1993.8570935726166s, Loss:2.5476\n",
      "Epoch 1,Step [6697/12942],1999.22061252594s, Loss:1.9296\n",
      "Epoch 1,Step [6698/12942],2004.2570431232452s, Loss:2.5045\n",
      "Epoch 1,Step [6699/12942],2013.4847629070282s, Loss:2.0274\n",
      "Epoch 1,Step [6700/12942],2044.6430010795593s, Loss:2.7952\n",
      "Epoch 1,Step [6701/12942],2077.4414551258087s, Loss:2.1913\n",
      "Epoch 1,Step [6702/12942],2116.05699467659s, Loss:2.1895\n",
      "Epoch 1,Step [6703/12942],2148.112344264984s, Loss:2.0461\n",
      "Epoch 1,Step [6704/12942],2180.3731672763824s, Loss:2.6113\n",
      "Epoch 1,Step [6705/12942],2212.106350183487s, Loss:2.2362\n",
      "Epoch 1,Step [6706/12942],2244.8611390590668s, Loss:2.4486\n",
      "Epoch 1,Step [6707/12942],2275.3945899009705s, Loss:1.9148\n",
      "Epoch 1,Step [6708/12942],2280.960916042328s, Loss:2.0012\n",
      "Epoch 1,Step [6709/12942],2286.131740093231s, Loss:2.1809\n",
      "Epoch 1,Step [6710/12942],2291.4091985225677s, Loss:1.8615\n",
      "Epoch 1,Step [6711/12942],2296.7980172634125s, Loss:2.4238\n",
      "Epoch 1,Step [6712/12942],2301.895584344864s, Loss:1.9466\n",
      "Epoch 1,Step [6713/12942],2308.3814749717712s, Loss:2.0346\n",
      "Epoch 1,Step [6714/12942],2332.9883506298065s, Loss:2.0567\n",
      "Epoch 1,Step [6715/12942],2363.26824259758s, Loss:2.3683\n",
      "Epoch 1,Step [6716/12942],2395.411500453949s, Loss:2.1341\n",
      "Epoch 1,Step [6717/12942],2426.7835030555725s, Loss:2.3990\n",
      "Epoch 1,Step [6718/12942],2467.8633341789246s, Loss:2.2922\n",
      "Epoch 1,Step [6719/12942],2499.388186454773s, Loss:1.9978\n",
      "Epoch 1,Step [6720/12942],2533.3375129699707s, Loss:1.9378\n",
      "Epoch 1,Step [6721/12942],2566.5999789237976s, Loss:2.0157\n",
      "Epoch 1,Step [6722/12942],2577.9619793891907s, Loss:2.3038\n",
      "Epoch 1,Step [6723/12942],2583.212450027466s, Loss:2.2796\n",
      "Epoch 1,Step [6724/12942],2588.333500146866s, Loss:2.0165\n",
      "Epoch 1,Step [6725/12942],2593.5729370117188s, Loss:1.9583\n",
      "Epoch 1,Step [6726/12942],2598.840262413025s, Loss:2.1588\n",
      "Epoch 1,Step [6727/12942],2604.2628376483917s, Loss:2.9350\n",
      "Epoch 1,Step [6728/12942],2613.522475004196s, Loss:2.2046\n",
      "Epoch 1,Step [6729/12942],2654.380219936371s, Loss:1.9854\n",
      "Epoch 1,Step [6730/12942],2685.528112888336s, Loss:2.1002\n",
      "Epoch 1,Step [6731/12942],2718.538952589035s, Loss:1.9211\n",
      "Epoch 1,Step [6732/12942],2752.438981294632s, Loss:2.1890\n",
      "Epoch 1,Step [6733/12942],2785.382577896118s, Loss:2.2734\n",
      "Epoch 1,Step [6734/12942],2826.158972978592s, Loss:1.9600\n",
      "Epoch 1,Step [6735/12942],2856.4974126815796s, Loss:1.8971\n",
      "Epoch 1,Step [6736/12942],2876.0434284210205s, Loss:2.2451\n",
      "Epoch 1,Step [6737/12942],2881.2071273326874s, Loss:2.1771\n",
      "Epoch 1,Step [6738/12942],2886.5026733875275s, Loss:2.2877\n",
      "Epoch 1,Step [6739/12942],2891.7866015434265s, Loss:2.5542\n",
      "Epoch 1,Step [6740/12942],2897.0118429660797s, Loss:2.1876\n",
      "Epoch 1,Step [6741/12942],2902.248944044113s, Loss:2.2968\n",
      "Epoch 1,Step [6742/12942],2909.514799118042s, Loss:1.9310\n",
      "Epoch 1,Step [6743/12942],2932.8829231262207s, Loss:2.0881\n",
      "Epoch 1,Step [6744/12942],2965.2747082710266s, Loss:2.2140\n",
      "Epoch 1,Step [6745/12942],3006.3921926021576s, Loss:1.8238\n",
      "Epoch 1,Step [6746/12942],3038.0497913360596s, Loss:2.3514\n",
      "Epoch 1,Step [6747/12942],3070.915777206421s, Loss:2.2878\n",
      "Epoch 1,Step [6748/12942],3103.8043575286865s, Loss:1.7186\n",
      "Epoch 1,Step [6749/12942],3135.0329875946045s, Loss:2.6315\n",
      "Epoch 1,Step [6750/12942],3170.4353244304657s, Loss:2.2885\n",
      "Epoch 1,Step [6751/12942],3179.464670419693s, Loss:2.1706\n",
      "Epoch 1,Step [6752/12942],3184.6779420375824s, Loss:2.3005\n",
      "Epoch 1,Step [6753/12942],3190.00479388237s, Loss:2.4131\n",
      "Epoch 1,Step [6754/12942],3195.2518877983093s, Loss:2.5201\n",
      "Epoch 1,Step [6755/12942],3200.2416949272156s, Loss:2.1171\n",
      "Epoch 1,Step [6756/12942],3205.540687561035s, Loss:2.0971\n",
      "Epoch 1,Step [6757/12942],3215.8457639217377s, Loss:2.1442\n",
      "Epoch 1,Step [6758/12942],3248.0414776802063s, Loss:2.0718\n",
      "Epoch 1,Step [6759/12942],3277.8341240882874s, Loss:2.0731\n",
      "Epoch 1,Step [6760/12942],3310.756781578064s, Loss:2.1404\n",
      "Epoch 1,Step [6761/12942],3345.9369189739227s, Loss:2.2213\n",
      "Epoch 1,Step [6762/12942],3384.127818584442s, Loss:2.2004\n",
      "Epoch 1,Step [6763/12942],3417.06436419487s, Loss:2.0280\n",
      "Epoch 1,Step [6764/12942],3449.9655952453613s, Loss:1.9624\n",
      "Epoch 1,Step [6765/12942],3475.2721948623657s, Loss:2.4196\n",
      "Epoch 1,Step [6766/12942],3480.3254573345184s, Loss:2.5665\n",
      "Epoch 1,Step [6767/12942],3485.477105617523s, Loss:2.1333\n",
      "Epoch 1,Step [6768/12942],3490.623420238495s, Loss:2.2885\n",
      "Epoch 1,Step [6769/12942],3495.892961740494s, Loss:2.2940\n",
      "Epoch 1,Step [6770/12942],3501.1423428058624s, Loss:2.0216\n",
      "Epoch 1,Step [6771/12942],3507.4728446006775s, Loss:2.1681\n",
      "Epoch 1,Step [6772/12942],3527.237909555435s, Loss:2.2857\n",
      "Epoch 1,Step [6773/12942],3565.827934741974s, Loss:2.2955\n",
      "Epoch 1,Step [6774/12942],3599.226975917816s, Loss:1.9844\n",
      "Epoch 1,Step [6775/12942],3632.855304956436s, Loss:2.0924\n",
      "Epoch 1,Step [6776/12942],3666.4685175418854s, Loss:2.4564\n",
      "Epoch 1,Step [6777/12942],3697.8222930431366s, Loss:2.1304\n",
      "Epoch 1,Step [6778/12942],3736.8442583084106s, Loss:2.3518\n",
      "Epoch 1,Step [6779/12942],3768.2410006523132s, Loss:3.2348\n",
      "Epoch 1,Step [6780/12942],3778.36700797081s, Loss:2.2387\n",
      "Epoch 1,Step [6781/12942],3783.6806037425995s, Loss:1.9626\n",
      "Epoch 1,Step [6782/12942],3788.938492536545s, Loss:1.9288\n",
      "Epoch 1,Step [6783/12942],3794.1917247772217s, Loss:1.9989\n",
      "Epoch 1,Step [6784/12942],3799.413266658783s, Loss:2.2138\n",
      "Epoch 1,Step [6785/12942],3805.0611441135406s, Loss:2.1546\n",
      "Epoch 1,Step [6786/12942],3817.747420310974s, Loss:2.2141\n",
      "Epoch 1,Step [6787/12942],3848.791043281555s, Loss:2.2450\n",
      "Epoch 1,Step [6788/12942],3878.405692100525s, Loss:1.9526\n",
      "Epoch 1,Step [6789/12942],3917.1090002059937s, Loss:2.1542\n",
      "Epoch 1,Step [6790/12942],3949.681364297867s, Loss:2.2270\n",
      "Epoch 1,Step [6791/12942],3982.457234144211s, Loss:2.0654\n",
      "Epoch 1,Step [6792/12942],4012.064416408539s, Loss:2.0900\n",
      "Epoch 1,Step [6793/12942],4044.2739226818085s, Loss:2.8090\n",
      "Epoch 1,Step [6794/12942],4075.2991802692413s, Loss:1.8699\n",
      "Epoch 1,Step [6795/12942],4080.500630378723s, Loss:2.2519\n",
      "Epoch 1,Step [6796/12942],4085.727691888809s, Loss:2.0635\n",
      "Epoch 1,Step [6797/12942],4090.5904610157013s, Loss:2.2172\n",
      "Epoch 1,Step [6798/12942],4095.770164489746s, Loss:2.1189\n",
      "Epoch 1,Step [6799/12942],4101.037165880203s, Loss:2.0493\n",
      "Epoch 1,Step [6800/12942],4107.103715419769s, Loss:2.4338\n",
      "Epoch 1,Step [6801/12942],4124.896014690399s, Loss:2.7789\n",
      "Epoch 1,Step [6802/12942],4158.549020528793s, Loss:2.0313\n",
      "Epoch 1,Step [6803/12942],4191.295034170151s, Loss:2.3627\n",
      "Epoch 1,Step [6804/12942],4223.5293300151825s, Loss:2.1763\n",
      "Epoch 1,Step [6805/12942],4263.021106004715s, Loss:2.0507\n",
      "Epoch 1,Step [6806/12942],4294.096328496933s, Loss:2.3465\n",
      "Epoch 1,Step [6807/12942],4327.383690357208s, Loss:2.1680\n",
      "Epoch 1,Step [6808/12942],4360.848854064941s, Loss:2.2128\n",
      "Epoch 1,Step [6809/12942],4377.0285086631775s, Loss:2.0235\n",
      "Epoch 1,Step [6810/12942],4382.237543344498s, Loss:2.2078\n",
      "Epoch 1,Step [6811/12942],4387.536568403244s, Loss:2.6063\n",
      "Epoch 1,Step [6812/12942],4392.784798622131s, Loss:2.4430\n",
      "Epoch 1,Step [6813/12942],4398.050359964371s, Loss:1.7635\n",
      "Epoch 1,Step [6814/12942],4403.043051242828s, Loss:2.2049\n",
      "Epoch 1,Step [6815/12942],4411.074910879135s, Loss:3.0145\n",
      "Epoch 1,Step [6816/12942],4445.886108636856s, Loss:2.4183\n",
      "Epoch 1,Step [6817/12942],4479.222082614899s, Loss:2.3370\n",
      "Epoch 1,Step [6818/12942],4507.332472324371s, Loss:2.0559\n",
      "Epoch 1,Step [6819/12942],4538.128803253174s, Loss:2.0993\n",
      "Epoch 1,Step [6820/12942],4571.226419687271s, Loss:2.3182\n",
      "Epoch 1,Step [6821/12942],4603.855933904648s, Loss:2.0569\n",
      "Epoch 1,Step [6822/12942],4640.319458246231s, Loss:2.2330\n",
      "Epoch 1,Step [6823/12942],4671.841580867767s, Loss:2.0188\n",
      "Epoch 1,Step [6824/12942],4678.6477699279785s, Loss:2.1476\n",
      "Epoch 1,Step [6825/12942],4683.939081430435s, Loss:2.1789\n",
      "Epoch 1,Step [6826/12942],4688.99532866478s, Loss:2.2427\n",
      "Epoch 1,Step [6827/12942],4693.858417510986s, Loss:2.0845\n",
      "Epoch 1,Step [6828/12942],4698.950772047043s, Loss:1.9834\n",
      "Epoch 1,Step [6829/12942],4704.334233999252s, Loss:2.0730\n",
      "Epoch 1,Step [6830/12942],4713.717032670975s, Loss:2.2210\n",
      "Epoch 1,Step [6831/12942],4746.331389427185s, Loss:2.2024\n",
      "Epoch 1,Step [6832/12942],4777.24191570282s, Loss:2.3331\n",
      "Epoch 1,Step [6833/12942],4818.1017689704895s, Loss:2.0486\n",
      "Epoch 1,Step [6834/12942],4850.8567707538605s, Loss:2.4893\n",
      "Epoch 1,Step [6835/12942],4882.850458145142s, Loss:2.1536\n",
      "Epoch 1,Step [6836/12942],4916.30571603775s, Loss:2.3114\n",
      "Epoch 1,Step [6837/12942],4949.271909236908s, Loss:1.9919\n",
      "Epoch 1,Step [6838/12942],4976.251536607742s, Loss:1.9297\n",
      "Epoch 1,Step [6839/12942],4981.611181497574s, Loss:2.1968\n",
      "Epoch 1,Step [6840/12942],4986.8764481544495s, Loss:2.6511\n",
      "Epoch 1,Step [6841/12942],4992.189508676529s, Loss:2.1055\n",
      "Epoch 1,Step [6842/12942],4997.443465948105s, Loss:2.4404\n",
      "Epoch 1,Step [6843/12942],5002.439056158066s, Loss:2.6998\n",
      "Epoch 1,Step [6844/12942],5008.967797756195s, Loss:2.2649\n",
      "Epoch 1,Step [6845/12942],5032.831023454666s, Loss:2.0474\n",
      "Epoch 1,Step [6846/12942],5063.158161640167s, Loss:2.1136\n",
      "Epoch 1,Step [6847/12942],5094.643827915192s, Loss:2.1705\n",
      "Epoch 1,Step [6848/12942],5126.603925704956s, Loss:2.1265\n",
      "Epoch 1,Step [6849/12942],5169.546576976776s, Loss:2.0697\n",
      "Epoch 1,Step [6850/12942],5202.7643575668335s, Loss:2.1175\n",
      "Epoch 1,Step [6851/12942],5235.964725017548s, Loss:2.2624\n",
      "Epoch 1,Step [6852/12942],5267.56013584137s, Loss:2.1091\n",
      "Epoch 1,Step [6853/12942],5277.881928443909s, Loss:2.1487\n",
      "Epoch 1,Step [6854/12942],5282.884891986847s, Loss:2.0263\n",
      "Epoch 1,Step [6855/12942],5288.166579723358s, Loss:2.1932\n",
      "Epoch 1,Step [6856/12942],5293.401563167572s, Loss:1.9104\n",
      "Epoch 1,Step [6857/12942],5298.615655660629s, Loss:2.3060\n",
      "Epoch 1,Step [6858/12942],5303.800372123718s, Loss:2.4488\n",
      "Epoch 1,Step [6859/12942],5312.86362361908s, Loss:2.2479\n",
      "Epoch 1,Step [6860/12942],5352.114799022675s, Loss:2.3809\n",
      "Epoch 1,Step [6861/12942],5385.413451194763s, Loss:2.0993\n",
      "Epoch 1,Step [6862/12942],5415.789658546448s, Loss:2.5105\n",
      "Epoch 1,Step [6863/12942],5447.482746362686s, Loss:2.2278\n",
      "Epoch 1,Step [6864/12942],5480.14755487442s, Loss:2.0348\n",
      "Epoch 1,Step [6865/12942],5519.792248010635s, Loss:2.2156\n",
      "Epoch 1,Step [6866/12942],5553.392758369446s, Loss:2.4645\n",
      "Epoch 1,Step [6867/12942],5575.604996204376s, Loss:2.3425\n",
      "Epoch 1,Step [6868/12942],5580.834978818893s, Loss:2.2255\n",
      "Epoch 1,Step [6869/12942],5586.062572956085s, Loss:2.2163\n",
      "Epoch 1,Step [6870/12942],5591.248865604401s, Loss:2.1919\n",
      "Epoch 1,Step [6871/12942],5596.496755123138s, Loss:2.2055\n",
      "Epoch 1,Step [6872/12942],5601.729904651642s, Loss:2.3154\n",
      "Epoch 1,Step [6873/12942],5608.171624422073s, Loss:2.3360\n",
      "Epoch 1,Step [6874/12942],5630.412412405014s, Loss:2.2770\n",
      "Epoch 1,Step [6875/12942],5662.39932513237s, Loss:2.2311\n",
      "Epoch 1,Step [6876/12942],5702.549516439438s, Loss:2.1848\n",
      "Epoch 1,Step [6877/12942],5735.150784730911s, Loss:2.1594\n",
      "Epoch 1,Step [6878/12942],5767.43638420105s, Loss:2.0550\n",
      "Epoch 1,Step [6879/12942],5798.934195280075s, Loss:2.0167\n",
      "Epoch 1,Step [6880/12942],5830.814427614212s, Loss:2.0951\n",
      "Epoch 1,Step [6881/12942],5863.23988199234s, Loss:2.1247\n",
      "Epoch 1,Step [6882/12942],5878.61475777626s, Loss:2.2297\n",
      "Epoch 1,Step [6883/12942],5883.593210935593s, Loss:2.2538\n",
      "Epoch 1,Step [6884/12942],5888.883471250534s, Loss:2.3283\n",
      "Epoch 1,Step [6885/12942],5894.112355470657s, Loss:2.2854\n",
      "Epoch 1,Step [6886/12942],5899.303297758102s, Loss:2.2457\n",
      "Epoch 1,Step [6887/12942],5904.672768115997s, Loss:2.1494\n",
      "Epoch 1,Step [6888/12942],5915.215332269669s, Loss:2.3812\n",
      "Epoch 1,Step [6889/12942],5946.673881292343s, Loss:2.4099\n",
      "Epoch 1,Step [6890/12942],5977.95022892952s, Loss:1.9896\n",
      "Epoch 1,Step [6891/12942],6010.933044672012s, Loss:1.8625\n",
      "Epoch 1,Step [6892/12942],6043.411883831024s, Loss:2.2205\n",
      "Epoch 1,Step [6893/12942],6086.159576654434s, Loss:2.3097\n",
      "Epoch 1,Step [6894/12942],6121.664311885834s, Loss:2.8458\n"
     ]
    }
   ],
   "source": [
    "# Load the last checkpoints\n",
    "rem_epochs = 1\n",
    "checkpoint = torch.load(os.path.join('./models','all_set' 'train-model-1-10500.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Load start_loss from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_loss = checkpoint['total_loss']\n",
    "# Reset start_loss to 0.0 if starting a new epoch; otherwise comment it out\n",
    "#start_loss = 0.0\n",
    "\n",
    "# Load epoch. Add 1 if we start a new epoch\n",
    "epoch = checkpoint['epoch']\n",
    "# Load start_step from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_step = checkpoint['train_step'] + 1\n",
    "# Reset start_step to 1 if starting a new epoch; otherwise comment it out\n",
    "#start_step = 1\n",
    "\n",
    "# # Train 1 epoch at a time due to very long training time\n",
    "# train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "#                    vocab_size, epoch, total_train_step, start_step, start_loss)\n",
    "start_time = time.time()\n",
    "for epoch in range(epoch, epoch+rem_epoch + 1):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "             break\n",
    "    start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
